{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-level control through deep reinforcement learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝 세미나 : 코드리뷰 [1, 2, 8]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* Paper Review\n",
    "* Code Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] Playing Atari With Deep Reinforcement Learning (NIPS 2013) 논문리뷰 - http://sanghyukchun.github.io/90/\n",
    "* [4] RL slide - https://computing.ece.vt.edu/~f15ece6504/slides/L26_RL.pdf\n",
    "* [12] Deep Reinforcement Learning - ICLR 2015 tutorial - http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf\n",
    "* [11] Deep Q-Learning - http://www.slideshare.net/nikolaypavlov/deep-qlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sanghyukchun.github.io/images/post/90-6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/rl-presentation-160522151115/95/deep-qlearning-9-1024.jpg?cb=1463930058\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/rl-presentation-160522151115/95/deep-qlearning-10-1024.jpg?cb=1463930058\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a deep convolutional neural network to approximate the optimal action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform experience replay we store \n",
    "\n",
    "the agent’s experiences \n",
    "\n",
    "$e_t = (s_t,a_t,r_t,s_{t+1})$ at each time-step $t$ \n",
    "\n",
    "in a data set \n",
    "\n",
    "$D_t$ =  {$e_1$,...,$e_t$}. \n",
    "\n",
    "During learning, we apply Q-learning updates, \n",
    "\n",
    "on samples (or minibatches) of experience \n",
    "\n",
    "$(s,a,r,s')$ ~ $U(D)$, \n",
    "\n",
    "drawn uniformly at random from the pool of stored samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning update at iteration i uses the following loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.2.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.10.png\" width=1000 />\n",
    "<img src=\"figures/capn.11.png\" width=600 />\n",
    "<img src=\"figures/capn.12.png\" width=600 />\n",
    "<img src=\"figures/capn.13.png\" width=1000 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.3.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.4.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [10] Distributed Deep Q-Learning - http://www.slideshare.net/onghaoyi/distributed-deep-qlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/dist-deep-qlearn-slides-151023232840-lva1-app6891/95/distributed-deep-qlearning-13-1024.jpg?cb=1445643171\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [5] songrotek's code - https://github.com/songrotek/DQN-Atari-Tensorflow\n",
    "* [6] asrivat1's code - https://github.com/asrivat1/DeepLearningVideoGames\n",
    "* [7] gliese581gg's code - https://github.com/gliese581gg/DQN_tensorflow\n",
    "* [8] devsisters' code - https://github.com/devsisters/DQN-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code can be accessed at https://sites.google.com/a/ deepmind.com/dqn for non-commercial uses only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/dist-deep-qlearn-slides-151023232840-lva1-app6891/95/distributed-deep-qlearning-14-638.jpg?cb=1445643171\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://sanghyukchun.github.io/images/post/90-5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.9.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to a sequence of loss functions $L_i(\\theta_i)$ that changes at each iteration $i$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating the loss function with respect to the weights we arrive at the following gradient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training algorithm for deep Q-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/eqn.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.5.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.6.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.7.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.8.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/capn.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 코드를 리뷰한다 \n",
    "* [8] devsisters' code - https://github.com/devsisters/DQN-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t\t\tassets\t     config.pyc  main.py\r\n",
      "README.md\t\t\tcheckpoints  dqn\t paper\r\n",
      "Render_OpenAI_gym_as_GIF.ipynb\tconfig.py    logs\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To train a model for Breakout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python main.py --env_name=Breakout-v0 --is_train=True --display=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py<br>\n",
    " -- dqn/agent.py<br>\n",
    " -- dqn/environment.py<br> \n",
    " -- config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from dqn.agent import Agent\n",
    "from dqn.environment import GymEnvironment, SimpleGymEnvironment\n",
    "from config import get_config\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "# Model\n",
    "flags.DEFINE_string('model', 'm1', 'Type of model')\n",
    "flags.DEFINE_boolean('dueling', False, 'Whether to use dueling deep q-network')\n",
    "flags.DEFINE_boolean('double_q', False, 'Whether to use double q-learning')\n",
    "\n",
    "# Environment\n",
    "flags.DEFINE_string('env_name', 'Breakout-v0', 'The name of gym environment to use')\n",
    "flags.DEFINE_integer('action_repeat', 4, 'The number of action to be repeated')\n",
    "\n",
    "# Etc\n",
    "flags.DEFINE_boolean('use_gpu', True, 'Whether to use gpu or not')\n",
    "flags.DEFINE_string('gpu_fraction', '1/1', 'idx / # of gpu fraction e.g. 1/3, 2/3, 3/3')\n",
    "flags.DEFINE_boolean('display', False, 'Whether to do display the game screen or not')\n",
    "flags.DEFINE_boolean('is_train', True, 'Whether to do training or testing')\n",
    "flags.DEFINE_integer('random_seed', 123, 'Value of random seed')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Set random seed\n",
    "tf.set_random_seed(FLAGS.random_seed)\n",
    "random.seed(FLAGS.random_seed)\n",
    "\n",
    "if FLAGS.gpu_fraction == '':\n",
    "  raise ValueError(\"--gpu_fraction should be defined\")\n",
    "\n",
    "def calc_gpu_fraction(fraction_string):\n",
    "  idx, num = fraction_string.split('/')\n",
    "  idx, num = float(idx), float(num)\n",
    "\n",
    "  fraction = 1 / (num - idx + 1)\n",
    "  print \" [*] GPU : %.4f\" % fraction\n",
    "  return fraction\n",
    "\n",
    "def main(_):\n",
    "  gpu_options = tf.GPUOptions(\n",
    "      per_process_gpu_memory_fraction=calc_gpu_fraction(FLAGS.gpu_fraction))\n",
    "\n",
    "  with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    config = get_config(FLAGS) or FLAGS\n",
    "\n",
    "    if config.env_type == 'simple':\n",
    "      env = SimpleGymEnvironment(config)\n",
    "    else:\n",
    "      env = GymEnvironment(config)\n",
    "\n",
    "    if not FLAGS.use_gpu:\n",
    "      config.cnn_format = 'NHWC'\n",
    "\n",
    "    agent = Agent(config, env, sess)\n",
    "\n",
    "    if FLAGS.is_train:\n",
    "      agent.train()\n",
    "    else:\n",
    "      agent.play()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AgentConfig(object):\n",
    "  scale = 10000\n",
    "  display = False\n",
    "\n",
    "  max_step = 5000 * scale\n",
    "  memory_size = 100 * scale\n",
    "\n",
    "  ...\n",
    "\n",
    "\n",
    "class EnvironmentConfig(object):\n",
    "  ...\n",
    "\n",
    "\n",
    "class DQNConfig(AgentConfig, EnvironmentConfig):\n",
    "  model = ''\n",
    "  pass\n",
    "\n",
    "\n",
    "class M1(DQNConfig):\n",
    "  ...\n",
    "\n",
    "\n",
    "def get_config(FLAGS):\n",
    "  if FLAGS.model == 'm1':\n",
    "    config = M1\n",
    "  elif FLAGS.model == 'm2':\n",
    "    config = M2\n",
    "\n",
    "  for k, v in FLAGS.__dict__['__flags'].items():\n",
    "    if k == 'gpu':\n",
    "      if v == False:\n",
    "        config.cnn_format = 'NHWC'\n",
    "      else:\n",
    "        config.cnn_format = 'NCHW'\n",
    "\n",
    "    if hasattr(config, k):\n",
    "      setattr(config, k, v)\n",
    "\n",
    "  return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dqn/agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent.py <br>\n",
    "-- base.py <br>\n",
    "-- history.py <br>\n",
    "-- ops.py <br>\n",
    "-- replay_memory.py <br>\n",
    "-- utils.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from .base import BaseModel\n",
    "from .history import History\n",
    "from .ops import linear, conv2d\n",
    "from .replay_memory import ReplayMemory\n",
    "from utils import get_time, save_pkl, load_pkl\n",
    "\n",
    "class Agent(BaseModel):\n",
    "  def __init__(self, config, environment, sess):\n",
    "    super(Agent, self).__init__(config)\n",
    "    self.sess = sess\n",
    "    self.weight_dir = 'weights'\n",
    "\n",
    "    self.env = environment\n",
    "    self.history = History(self.config)\n",
    "    self.memory = ReplayMemory(self.config, self.model_dir)\n",
    "\n",
    "    ...\n",
    "\n",
    "    self.build_dqn()\n",
    "\n",
    "  def train(self):\n",
    "    \n",
    "    ...\n",
    "\n",
    "    num_game, self.update_count, ep_reward = 0, 0, 0.\n",
    "    total_reward, self.total_loss, self.total_q = 0., 0., 0.\n",
    "    max_avg_ep_reward = 0\n",
    "    ep_rewards, actions = [], []\n",
    "\n",
    "    screen, reward, action, terminal = self.env.new_random_game()\n",
    "\n",
    "    for _ in range(self.history_length):\n",
    "      self.history.add(screen)\n",
    "\n",
    "    for self.step in tqdm(range(start_step, self.max_step), ncols=70, initial=start_step):\n",
    "      \n",
    "      ...\n",
    "        \n",
    "      # 1. predict\n",
    "      action = self.predict(self.history.get())\n",
    "      # 2. act\n",
    "      screen, reward, terminal = self.env.act(action, is_training=True)\n",
    "      # 3. observe\n",
    "      self.observe(screen, reward, action, terminal)\n",
    "\n",
    "      if terminal:\n",
    "        screen, reward, action, terminal = self.env.new_random_game()\n",
    "\n",
    "        num_game += 1\n",
    "        ep_rewards.append(ep_reward)\n",
    "        ep_reward = 0.\n",
    "      else:\n",
    "        ep_reward += reward\n",
    "\n",
    "      actions.append(action)\n",
    "      total_reward += reward\n",
    "\n",
    "      if self.step >= self.learn_start:\n",
    "        if self.step % self.test_step == self.test_step - 1:\n",
    "          avg_reward = total_reward / self.test_step\n",
    "          avg_loss = self.total_loss / self.update_count\n",
    "          avg_q = self.total_q / self.update_count\n",
    "\n",
    "          try:\n",
    "            max_ep_reward = np.max(ep_rewards)\n",
    "            min_ep_reward = np.min(ep_rewards)\n",
    "            avg_ep_reward = np.mean(ep_rewards)\n",
    "          except:\n",
    "            max_ep_reward, min_ep_reward, avg_ep_reward = 0, 0, 0\n",
    "\n",
    "          ...\n",
    "        \n",
    "        \n",
    "          num_game = 0\n",
    "          total_reward = 0.\n",
    "          self.total_loss = 0.\n",
    "          self.total_q = 0.\n",
    "          self.update_count = 0\n",
    "          ep_reward = 0.\n",
    "          ep_rewards = []\n",
    "          actions = []\n",
    "\n",
    "  def predict(self, s_t, test_ep=None):\n",
    "    ...\n",
    "\n",
    "    return action\n",
    "\n",
    "  def observe(self, screen, reward, action, terminal):\n",
    "    ...\n",
    "\n",
    "  def q_learning_mini_batch(self):\n",
    "    ..\n",
    "    \n",
    "  def build_dqn(self):\n",
    "    self.w = {}\n",
    "    self.t_w = {}\n",
    "\n",
    "    #initializer = tf.contrib.layers.xavier_initializer()\n",
    "    initializer = tf.truncated_normal_initializer(0, 0.02)\n",
    "    activation_fn = tf.nn.relu\n",
    "\n",
    "    # training network\n",
    "    with tf.variable_scope('prediction'):\n",
    "       ...\n",
    "    \n",
    "    # target network\n",
    "    with tf.variable_scope('target'):\n",
    "       ...\n",
    "    \n",
    "    with tf.variable_scope('pred_to_target'):\n",
    "       ...\n",
    "    \n",
    "    \n",
    "    # optimizer\n",
    "    with tf.variable_scope('optimizer'):\n",
    "      self.optim = tf.train.RMSPropOptimizer(\n",
    "          self.learning_rate_op, momentum=0.95, epsilon=0.01).minimize(self.loss)\n",
    "\n",
    "    with tf.variable_scope('summary'):\n",
    "      scalar_summary_tags = ['average.reward', 'average.loss', 'average.q', \\\n",
    "          'episode.max reward', 'episode.min reward', 'episode.avg reward', 'episode.num of game', 'training.learning_rate']\n",
    "\n",
    "      ...\n",
    "        \n",
    "      histogram_summary_tags = ['episode.rewards', 'episode.actions']\n",
    "\n",
    "      ...\n",
    "\n",
    "      self.writer = tf.train.SummaryWriter('./logs/%s' % self.model_dir, self.sess.graph)\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    self._saver = tf.train.Saver(self.w.values() + [self.step_op], max_to_keep=30)\n",
    "\n",
    "    self.load_model()\n",
    "    self.update_target_q_network()\n",
    "\n",
    "    \n",
    " \n",
    "  def play(self, n_step=10000, n_episode=100, test_ep=None, render=False):\n",
    "    if test_ep == None:\n",
    "      test_ep = self.ep_end\n",
    "\n",
    "    test_history = History(self.config)\n",
    "\n",
    "    if not self.display:\n",
    "      gym_dir = '/tmp/%s-%s' % (self.env_name, get_time())\n",
    "      self.env.env.monitor.start(gym_dir)\n",
    "\n",
    "    best_reward, best_idx = 0, 0\n",
    "    for idx in xrange(n_episode):\n",
    "      screen, reward, action, terminal = self.env.new_random_game()\n",
    "      current_reward = 0\n",
    "\n",
    "      for _ in range(self.history_length):\n",
    "        test_history.add(screen)\n",
    "\n",
    "      for t in tqdm(range(n_step), ncols=70):\n",
    "        # 1. predict\n",
    "        action = self.predict(test_history.get(), test_ep)\n",
    "        # 2. act\n",
    "        screen, reward, terminal = self.env.act(action, is_training=False)\n",
    "        # 3. observe\n",
    "        test_history.add(screen)\n",
    "\n",
    "        current_reward += reward\n",
    "        if terminal:\n",
    "          break\n",
    "\n",
    "      if current_reward > best_reward:\n",
    "        best_reward = current_reward\n",
    "        best_idx = idx\n",
    "\n",
    "      print \"=\"*30\n",
    "      print \" [%d] Best reward : %d\" % (best_idx, best_reward)\n",
    "      print \"=\"*30\n",
    "\n",
    "    if not self.display:\n",
    "      self.env.env.monitor.close()\n",
    "      #gym.upload(gym_dir, writeup='https://github.com/devsisters/DQN-tensorflow', api_key='')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### base.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "  \"\"\"Abstract object representing an Reader model.\"\"\"\n",
    "  ...\n",
    "\n",
    "  def save_model(self, step=None):\n",
    "    ...\n",
    "    \n",
    "  def load_model(self):\n",
    "    ...\n",
    "    \n",
    "  @property\n",
    "  def checkpoint_dir(self):\n",
    "    return os.path.join('checkpoints', self.model_dir)\n",
    "\n",
    "  @property\n",
    "  def model_dir(self):\n",
    "    ...\n",
    "    return model_dir + '/'\n",
    "\n",
    "  @property\n",
    "  def saver(self):\n",
    "    ...\n",
    "    return self._saver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### history.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class History:\n",
    "  def __init__(self, config):\n",
    "    self.cnn_format = config.cnn_format\n",
    "\n",
    "    batch_size, history_length, screen_height, screen_width = \\\n",
    "        config.batch_size, config.history_length, config.screen_height, config.screen_width\n",
    "\n",
    "    self.history = np.zeros(\n",
    "        [history_length, screen_height, screen_width], dtype=np.float32)\n",
    "\n",
    "  def add(self, screen):\n",
    "    self.history[:-1] = self.history[1:]\n",
    "    self.history[-1] = screen\n",
    "\n",
    "  def reset(self):\n",
    "    self.history *= 0\n",
    "\n",
    "  def get(self):\n",
    "    if self.cnn_format == 'NHWC':\n",
    "      return np.transpose(self.history, (1, 2, 0))\n",
    "    else:\n",
    "      return self.history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ops.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x,\n",
    "           output_dim,\n",
    "           kernel_size,\n",
    "           stride,\n",
    "           initializer=tf.contrib.layers.xavier_initializer(),\n",
    "           activation_fn=tf.nn.relu,\n",
    "           data_format='NHWC',\n",
    "           padding='VALID',\n",
    "           name='conv2d'):\n",
    "  with tf.variable_scope(name):\n",
    "    if data_format == 'NCHW':\n",
    "      stride = [1, 1, stride[0], stride[1]]\n",
    "      kernel_shape = [kernel_size[0], kernel_size[1], x.get_shape()[1], output_dim]\n",
    "    elif data_format == 'NHWC':\n",
    "      stride = [1, stride[0], stride[1], 1]\n",
    "      kernel_shape = [kernel_size[0], kernel_size[1], x.get_shape()[-1], output_dim]\n",
    "\n",
    "    w = tf.get_variable('w', kernel_shape, tf.float32, initializer=initializer)\n",
    "    conv = tf.nn.conv2d(x, w, stride, padding, data_format=data_format)\n",
    "\n",
    "    b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "    out = tf.nn.bias_add(conv, b, data_format)\n",
    "\n",
    "  if activation_fn != None:\n",
    "    out = activation_fn(out)\n",
    "\n",
    "  return out, w, b\n",
    "\n",
    "def linear(input_, output_size, stddev=0.02, bias_start=0.0, activation_fn=None, name='linear'):\n",
    "  shape = input_.get_shape().as_list()\n",
    "\n",
    "  with tf.variable_scope(name):\n",
    "    w = tf.get_variable('Matrix', [shape[1], output_size], tf.float32,\n",
    "        tf.random_normal_initializer(stddev=stddev))\n",
    "    b = tf.get_variable('bias', [output_size],\n",
    "        initializer=tf.constant_initializer(bias_start))\n",
    "\n",
    "    out = tf.nn.bias_add(tf.matmul(input_, w), b)\n",
    "\n",
    "    if activation_fn != None:\n",
    "      return activation_fn(out), w, b\n",
    "    else:\n",
    "      return out, w, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replay_memory.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Code from https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py\"\"\"\n",
    "\n",
    "class ReplayMemory:\n",
    "  def __init__(self, config, model_dir):\n",
    "    self.model_dir = model_dir\n",
    "\n",
    "    self.cnn_format = config.cnn_format\n",
    "    self.memory_size = config.memory_size\n",
    "    self.actions = np.empty(self.memory_size, dtype = np.uint8)\n",
    "    self.rewards = np.empty(self.memory_size, dtype = np.integer)\n",
    "    self.screens = np.empty((self.memory_size, config.screen_height, config.screen_width), dtype = np.float16)\n",
    "    self.terminals = np.empty(self.memory_size, dtype = np.bool)\n",
    "    self.history_length = config.history_length\n",
    "    self.dims = (config.screen_height, config.screen_width)\n",
    "    self.batch_size = config.batch_size\n",
    "    self.count = 0\n",
    "    self.current = 0\n",
    "\n",
    "    # pre-allocate prestates and poststates for minibatch\n",
    "    self.prestates = np.empty((self.batch_size, self.history_length) + self.dims, dtype = np.float16)\n",
    "    self.poststates = np.empty((self.batch_size, self.history_length) + self.dims, dtype = np.float16)\n",
    "\n",
    "  def add(self, screen, reward, action, terminal):\n",
    "    assert screen.shape == self.dims\n",
    "    # NB! screen is post-state, after action and reward\n",
    "    self.actions[self.current] = action\n",
    "    self.rewards[self.current] = reward\n",
    "    self.screens[self.current, ...] = screen\n",
    "    self.terminals[self.current] = terminal\n",
    "    self.count = max(self.count, self.current + 1)\n",
    "    self.current = (self.current + 1) % self.memory_size\n",
    "\n",
    "  def getState(self, index):\n",
    "    assert self.count > 0, \"replay memory is empy, use at least --random_steps 1\"\n",
    "    # normalize index to expected range, allows negative indexes\n",
    "    index = index % self.count\n",
    "    # if is not in the beginning of matrix\n",
    "    if index >= self.history_length - 1:\n",
    "      # use faster slicing\n",
    "      return self.screens[(index - (self.history_length - 1)):(index + 1), ...]\n",
    "    else:\n",
    "      # otherwise normalize indexes and use slower list based access\n",
    "      indexes = [(index - i) % self.count for i in reversed(range(self.history_length))]\n",
    "      return self.screens[indexes, ...]\n",
    "\n",
    "  def sample(self):\n",
    "    # memory must include poststate, prestate and history\n",
    "    assert self.count > self.history_length\n",
    "    # sample random indexes\n",
    "    indexes = []\n",
    "    while len(indexes) < self.batch_size:\n",
    "      # find random index \n",
    "      while True:\n",
    "        # sample one index (ignore states wraping over \n",
    "        index = random.randint(self.history_length, self.count - 1)\n",
    "        # if wraps over current pointer, then get new one\n",
    "        if index >= self.current and index - self.history_length < self.current:\n",
    "          continue\n",
    "        # if wraps over episode end, then get new one\n",
    "        # NB! poststate (last screen) can be terminal state!\n",
    "        if self.terminals[(index - self.history_length):index].any():\n",
    "          continue\n",
    "        # otherwise use this index\n",
    "        break\n",
    "      \n",
    "      # NB! having index first is fastest in C-order matrices\n",
    "      self.prestates[len(indexes), ...] = self.getState(index - 1)\n",
    "      self.poststates[len(indexes), ...] = self.getState(index)\n",
    "      indexes.append(index)\n",
    "\n",
    "    actions = self.actions[indexes]\n",
    "    rewards = self.rewards[indexes]\n",
    "    terminals = self.terminals[indexes]\n",
    "\n",
    "    if self.cnn_format == 'NHWC':\n",
    "      return np.transpose(self.prestates, (0, 2, 3, 1)), actions, \\\n",
    "        rewards, np.transpose(self.poststates, (0, 2, 3, 1)), terminals\n",
    "    else:\n",
    "      return self.prestates, actions, rewards, self.poststates, terminals\n",
    "\n",
    "  def save(self):\n",
    "    ...\n",
    "    \n",
    "  def load(self):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeit(f):\n",
    "  ...\n",
    "\n",
    "def get_time():\n",
    "  ...\n",
    "\n",
    "@timeit\n",
    "def save_pkl(obj, path):\n",
    "  ...\n",
    "\n",
    "@timeit\n",
    "def load_pkl(path):\n",
    "  ...\n",
    "\n",
    "@timeit\n",
    "def save_npy(obj, path):\n",
    "  ...\n",
    "\n",
    "@timeit\n",
    "def load_npy(path):\n",
    "  ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dqn/environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Environment(object):\n",
    "  def __init__(self, config):\n",
    "    ...\n",
    "    \n",
    "  def new_game(self, from_random_game=False):\n",
    "    ...\n",
    "    return self.screen, 0, 0, self.terminal\n",
    "\n",
    "  def new_random_game(self):\n",
    "    ...\n",
    "    return self.screen, 0, 0, self.terminal\n",
    "\n",
    "  def _step(self, action):\n",
    "    self._screen, self.reward, self.terminal, _ = self.env.step(action)\n",
    "\n",
    "  def _random_step(self):\n",
    "    action = self.env.action_space.sample()\n",
    "    self._step(action)\n",
    "\n",
    "  @ property\n",
    "  def screen(self):\n",
    "    return cv2.resize(cv2.cvtColor(self._screen, cv2.COLOR_RGB2GRAY)/255., self.dims)\n",
    "    #return cv2.resize(cv2.cvtColor(self._screen, cv2.COLOR_BGR2YCR_CB)/255., self.dims)[:,:,0]\n",
    "\n",
    "  @property\n",
    "  def action_size(self):\n",
    "    return self.env.action_space.n\n",
    "\n",
    "  @property\n",
    "  def lives(self):\n",
    "    return self.env.ale.lives()\n",
    "\n",
    "  @property\n",
    "  def state(self):\n",
    "    return self.screen, self.reward, self.terminal\n",
    "\n",
    "  def render(self):\n",
    "    if self.display:\n",
    "      self.env.render()\n",
    "\n",
    "  def after_act(self, action):\n",
    "    self.render()\n",
    "\n",
    "class GymEnvironment(Environment):\n",
    "  def __init__(self, config):\n",
    "    super(GymEnvironment, self).__init__(config)\n",
    "\n",
    "  def act(self, action, is_training=True):\n",
    "    ...\n",
    "\n",
    "class SimpleGymEnvironment(Environment):\n",
    "  def __init__(self, config):\n",
    "    super(SimpleGymEnvironment, self).__init__(config)\n",
    "\n",
    "  def act(self, action, is_training=True):\n",
    "    ...\n",
    "    return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] Playing Atari With Deep Reinforcement Learning - http://arxiv.org/abs/1312.5602\n",
    "* [2] Human-level control through deep reinforcement learning - http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf\n",
    "* [3] Playing Atari With Deep Reinforcement Learning (NIPS 2013) 논문리뷰 -  http://sanghyukchun.github.io/90/\n",
    "* [4] RL slide - https://computing.ece.vt.edu/~f15ece6504/slides/L26_RL.pdf\n",
    "* [5] songrotek's code - https://github.com/songrotek/DQN-Atari-Tensorflow\n",
    "* [6] asrivat1's code - https://github.com/asrivat1/DeepLearningVideoGames\n",
    "* [7] gliese581gg's code - https://github.com/gliese581gg/DQN_tensorflow\n",
    "* [8] devsisters' code - https://github.com/devsisters/DQN-tensorflow/\n",
    "* [9] 강화학습 그리고 OpenAI - http://www.modulabs.co.kr/RL_library/3237\n",
    "* [10] Distributed Deep Q-Learning - http://www.slideshare.net/onghaoyi/distributed-deep-qlearning\n",
    "* [11] Deep Q-Learning - http://www.slideshare.net/nikolaypavlov/deep-qlearning\n",
    "* [12] Deep Reinforcement Learning - ICLR 2015 tutorial - http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf\n",
    "* [13] Dueling Deep Q-Networks - http://torch.ch/blog/2016/04/30/dueling_dqn.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
