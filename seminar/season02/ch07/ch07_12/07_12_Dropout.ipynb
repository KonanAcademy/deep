{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 7. REGULARIZATION FOR DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝 세미나 : 이론 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 7.1 Parameter Norm Penalties\n",
    "* 7.2 Norm Penalties as Constrained Optimization\n",
    "* 7.3 Regularization and Under-Constrained Problems\n",
    "* 7.4 Dataset Augmentation\n",
    "* 7.5 Noise Robustness\n",
    "    - 7.5.1 Injecting Noise at the Output Targets\n",
    "* 7.6 Semi-Supervised Learning\n",
    "* 7.7 Multi-Task Learning\n",
    "* 7.8 Early Stopping\n",
    "* 7.9 Parameter Tying and Parameter Sharing\n",
    "* 7.10 Sparse Representations\n",
    "* 7.11 Bagging and Other Ensemble Methods\n",
    "* <font color=\"red\">7.12 Dropout</font>\n",
    "* 7.13 Adversarial Training\n",
    "* 7.14 Tangent Distance, Tangent Prop, and ManifoldTangent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] Training Neural Networks Part 2: parameter updates, ensembles, dropout Convolutional Neural Networks: intro - http://cs231n.stanford.edu/slides/winter1516_lecture6.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.12 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dropout(Srivastava et al., 2014) provides a computationally inexpensive but powerful method of regularizing a broad family of models.\n",
    "* Speciﬁcally, dropout trains the ensemble consisting of all sub-networks that can be formed by removing non-output units from an underlying base network,as illustrated in ﬁgure 7.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.1.png\" width=600 />\n",
    "<img src=\"figures/cap7.12.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In most modern neural networks, based on a series of aﬃne transformations and nonlinearities, we can eﬀectively remove a unit from anetwork by multiplying its output value by zero.\n",
    "* Typically,an input unit is included with probability 0.8 and a hidden unit is included with probability 0.5. \n",
    "* We then run forward propagation, back-propagation, and the learning update as usual. \n",
    "* Figure 7.7 illustrates how to run forward propagation with dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.3.png\" width=200 />\n",
    "<img src=\"figures/cap7.12.4.png\" width=400 />\n",
    "<img src=\"figures/cap7.12.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost function\n",
    "* More formally, suppose that a mask vector $µ$ speciﬁes which units to include, and $J(θ, µ)$ deﬁnes the cost of the model deﬁned by parameters $θ$ and mask $µ$.\n",
    "* Then dropout training consists in minimizing $E_µJ(θ, µ)$.\n",
    "* The expectation contains exponentially many terms but we can obtain an unbiased estimate of its gradientby sampling values of $µ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters sharing\n",
    "* Dropout training is not quite the same as bagging training. In the case ofbagging, the models are all independent. \n",
    "* <font color=\"red\">In the case of dropout, the models share parameters, with each model inheriting a diﬀerent subset of parameters from the parent neural network</font>. \n",
    "* This parameter sharing makes it possible to represent an exponential number of models with a <font color=\"red\">tractable amount of memory</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training \n",
    "* In the case of dropout, typically most models are not explicitly trained at all—usually,the model is large enough that it would be infeasible to sample all possible sub-networks within the lifetime of the universe. \n",
    "* <font color=\"red\">Instead, a tiny fraction of the possible sub-networks are each trained for a single step, and the parameter sharing causesthe remaining sub-networks to arrive at good settings of the parameters</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference\n",
    "* To make a prediction, a bagged ensemble must accumulate votes from all of its members. \n",
    "* <font color=\"red\">We refer to this process as inferencein this context</font>. \n",
    "* So far, our description of bagging and dropout has <font color=\"red\">not required that the model be explicitly probabilistic</font>. \n",
    "* Now, we assume that the model’s role is to output a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">In the case of bagging</font>, each model $i$ produces a probability distribution $p^{(i)}(y|x)$. The prediction of the ensemble is given by the arithmetic mean of all of these distributions,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">In the case of dropout</font>, each sub-model deﬁned by mask vector $µ$ deﬁnes a probability distribution $p(y|x,µ)$. The arithmetic mean over all masks is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $p(µ)$ is the probability distribution that was used to sample $µ$ at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### approximation\n",
    "* Because this sum includes an exponential number of terms, <font color=\"red\">it is intractable to evaluate</font> except in cases where the structure of the model permits some form of simpliﬁcation.\n",
    "    - So far, deep neural nets are not known to permit any tractable simpliﬁcation.\n",
    "* <font color=\"red\">Instead, we can approximate the inference with sampling, by averaging together the output from many masks</font>. \n",
    "    - Even 10-20 masks are often suﬃcient to obtain good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### geometric mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, there is an even better approach, that <font color=\"red\">allows us to obtain a good approximation to the predictions of the entire ensemble</font>, at the <font color=\"blue\">cost of only one forward propagation</font>. \n",
    "* To do so, we change to <font color=\"red\">using the geometric mean rather than the arithmetic mean of the ensemble members’ predicted distributions</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 \n",
    "* [3] Geometric mean - https://en.wikipedia.org/wiki/Geometric_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geometric mean of multiple probability distributions is not guaranteed to bea probability distribution. \n",
    "\n",
    "To guarantee that the result is a probability distribution, <font color=\"red\">we impose the requirement that none of the sub-models assigns probability 0 to any event, and we renormalize the resulting distribution</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unnormalized probability distribution deﬁned directly by the geometric mean is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $d$ is the number of units that may be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions we must re-normalize the ensemble:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight scaling inference rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"red\">A key insight (Hinton et al., 2012c) involved in dropout is that we can approximate $p_{ensemble}$ by evaluating $p(y|x)$ in one model</font>: \n",
    "    - the model with all units, but with the weights going out of unit $i$ multiplied by the probability of including unit $i$. \n",
    "    - The motivation for this modiﬁcation is to capture the right expected value of the output from that unit. \n",
    "* We call this approach the <font color=\"red\">weight scaling inference rule</font>.\n",
    "     - There is not yet any theoretical argument for the accuracy of this approximate inference rule in deep nonlinear networks, but empirically it performs very well\n",
    "    - Because we usually use an inclusion probability of 1/2, the weight scaling rule usually amounts to dividing the weights by 2 at the end of training, and then using the model as usual.\n",
    "     - Another way to achieve the same result is to multiply the states of the units by 2 during training.\n",
    "     - Either way, the goal is to make sure that\n",
    "         - the <font color=\"red\">expected total input</font> to a unit <font color=\"red\">at test time</font> \n",
    "             - <font color=\"red\">is roughly the same</font> \n",
    "                 - as the expected total input to that unit <font color=\"red\">at train time</font>, \n",
    "                     - even though half the units at train time a remissing on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many classes of models that do <font color=\"red\">not have nonlinear hidden units, the weight scaling inference rule is exact</font>. \n",
    "\n",
    "<font color=\"blue\">For a simple example, consider a softmax regression classiﬁer</font> with n input variables represented by the vector v:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index into the family of sub-models by element-wise multiplication of the input with a binary vector d:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble predictor is deﬁned by re-normalizing the geometric mean over all ensemble members’ predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.12.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the weight scaling rule is only an approximation <font color=\"blue\">for deep models that have nonlinearities</font>. \n",
    "* <font color=\"red\">Though the approximation has not been theoretically characterized, it often works well, empirically</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight scaling rule & Monte Carlo approximations\n",
    "*  Goodfellowet al. (2013a) found experimentally that the weight scaling approximation can work better (in terms of classiﬁcation accuracy) than Monte Carlo approximations to theensemble predictor. \n",
    "* Gal and Ghahramani (2015) found that some models obtain better classiﬁcation accuracy using twenty samples and the Monte Carlo approximation. \n",
    "* <font color=\"red\">It appears that the optimal choice of inference approximation is problem-dependent</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### advantages of dropout\n",
    "* Srivastava et al. (2014) showed that dropout is more eﬀective than\n",
    "    - other standard computationally inexpensive regularizers, \n",
    "        - such as \n",
    "            - weight decay, \n",
    "            - ﬁlternorm constraints and \n",
    "            - sparse activity regularization. \n",
    "* Dropout may also be combined with other forms of regularization to yield a further improvement.\n",
    "* One advantage of dropout is that it is <font color=\"red\">very computationally cheap</font>.\n",
    "    - Using dropout during training requires onlyvO(n) computation per example per update, to generaten random binary numbers and multiply them by the state.\n",
    "* Another signiﬁcant advantage of dropout is that it <font color=\"red\">does not signiﬁcantly limit</font> \n",
    "    - the <font color=\"red\">type of model</font> or \n",
    "    - <font color=\"red\">training procedure</font> that can be used.\n",
    "    - Many other regularization strategies of comparable power impose more severe restrictions on the architecture of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost of dropout\n",
    "* Though the cost per-step of applying dropout to a speciﬁc model is negligible,the <font color=\"red\">cost of using dropout in a complete system can be signiﬁcant</font>. \n",
    "* Because dropout is a regularization technique, <font color=\"red\">it reduces the eﬀective capacity of a model</font>. \n",
    "* To oﬀset this eﬀect, <font color=\"red\">we must increase the size of the model</font>.\n",
    "* Typically the optimal validation set error is much lower when using dropout, but this comes at the cost of a much larger model and many more iterations of the training algorithm.\n",
    "* <font color=\"blue\">For very large datasets, regularization confers little reduction in generalization error</font>.\n",
    "* <font color=\"red\">In these cases, the computational cost of using dropout and larger models may outweigh the beneﬁt of regularization</font>.\n",
    "* <font color=\"blue\">When extremely few labeled training examples</font> are available, <font color=\"red\">dropout is less eﬀective</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dropout & weight decay\n",
    "* Wager et al. (2013) showed that, when applied to linear regression, dropout is equivalent to $L^2$ weight decay.\n",
    "    - Similar results hold for other linear models\n",
    "* For deepmodels, dropout is not equivalent to weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stochasticity & fast dropout & dropout boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stochasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The stochasticity used while training with dropout is not necessary for the approach’s success. \n",
    "* It is just a means of approximating the sum over all sub-models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fast dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wang and Manning (2013) derived analytical approximations to this marginalization. \n",
    "* Their approximation, known as <font color=\"red\">fast dropout</font> resulted in faster convergence time due to the reduced stochasticity in the computation of the gradient. \n",
    "    - This method can also be applied at test time, as a more principled(but also more computationally expensive) approximation to the average over all sub-networks than the weight scaling approximation.\n",
    "* <font color=\"red\">Fast dropout has been used to nearly match the performance of standard dropout on small neural network problems, but has not yet yielded a signiﬁcant improvement or been applied to a large problem</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dropout boosting\n",
    "\n",
    "###### 참고\n",
    "* [4] 앙상블 기법 4 - 부스팅 - http://ishuca.tistory.com/entry/%EC%95%99%EC%83%81%EB%B8%94-%EA%B8%B0%EB%B2%95-4-%EB%B6%80%EC%8A%A4%ED%8C%85\n",
    "\n",
    "\n",
    "* Just as stochasticity is not necessary to achieve the regularizing eﬀect of dropout, it is also not suﬃcient. \n",
    "* To demonstrate this, Warde-Farley et al. (2014)designed control experiments using a method called dropout boosting \n",
    "    - that they designed to use exactly the same mask noise as traditional dropout but lack its regularizing eﬀect. \n",
    "    - Dropout boosting trains the entire ensemble to jointly maximize the log-likelihood on the training set. \n",
    "    - In the same sense that traditional dropout is analogous to bagging, this approach is analogous to boosting. \n",
    "* As intended, experiments with dropout boosting show almost no regularization eﬀect compared to training the entire network as a single model. \n",
    "- <font color=\"red\">This demonstrates that the interpretation of dropout as bagging has value beyond the interpretation of dropout as robustness to noise</font>. \n",
    "- The regularization eﬀect of the bagged ensemble is only achieved when the stochastically sampled ensemble members are trained to perform well independently of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout & DropConnect\n",
    "* Dropout has inspired other stochastic approaches to training exponentiallylarge ensembles of models that share weights. \n",
    "* <font color=\"red\">DropConnect</font> is a special case of dropout where each product between a single scalar weight and a single hidden unit state is considered a unit that can be dropped\n",
    "* <font color=\"red\">Stochastic pooling</font> is a form of randomized pooling (see section 9.3) for building ensembles of convolutional networks with each convolutional network attending to diﬀerentspatial locations of each feature map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, dropout remains the most widely used implicit ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">One of the key insights of dropout is that training a network with stochastic behavior and making predictions by averaging over multiple stochastic decisions implements a form of bagging with parameter sharing</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] DEEP LEARNING (Yoshua Bengio)- http://www.deeplearningbook.org/\n",
    "* [2] Training Neural Networks Part 2: parameter updates, ensembles, dropout Convolutional Neural Networks: intro - http://cs2Geometric mean31n.stanford.edu/slides/winter1516_lecture6.pdf\n",
    "* [3] Geometric mean - https://en.wikipedia.org/wiki/Geometric_mean\n",
    "* [4] 앙상블 기법 4 - 부스팅 - http://ishuca.tistory.com/entry/%EC%95%99%EC%83%81%EB%B8%94-%EA%B8%B0%EB%B2%95-4-%EB%B6%80%EC%8A%A4%ED%8C%85"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
