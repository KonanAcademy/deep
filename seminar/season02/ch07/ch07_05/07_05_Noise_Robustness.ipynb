{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 7. REGULARIZATION FOR DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝 세미나 : 이론 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 7.1 Parameter Norm Penalties\n",
    "* 7.2 Norm Penalties as Constrained Optimization\n",
    "* 7.3 Regularization and Under-Constrained Problems\n",
    "* 7.4 Dataset Augmentation\n",
    "* <font color=\"red\">7.5 Noise Robustness</font>\n",
    "    - 7.5.1 Injecting Noise at the Output Targets\n",
    "* 7.6 Semi-Supervised Learning\n",
    "* 7.7 Multi-Task Learning\n",
    "* 7.8 Early Stopping\n",
    "* 7.9 Parameter Tying and Parameter Sharing\n",
    "* 7.10 Sparse Representations\n",
    "* 7.11 Bagging and Other Ensemble Methods\n",
    "* 7.12 Dropout\n",
    "* 7.13 Adversarial Training\n",
    "* 7.14 Tangent Distance, Tangent Prop, and ManifoldTangent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5 Noise Robustness\n",
    "* 7.5.1 Injecting Noise at the Output Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inputs\n",
    "* Section 7.4 has motivated the use of noise applied to the inputs as a dataset augmentation strategy. \n",
    "* For some models, the addition of noise with inﬁnitesimal variance at the input of the model is equivalent to imposing a penalty on the norm of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden units\n",
    "In the general case, it is important to remember that <font color=\"red\">noise injection</font> can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bayesian\n",
    "* Another way that noise has been used in the service of regularizing models is by adding it to the weights. \n",
    "* This technique has been used primarily in thecontext of recurrent neural networks (Jim et al., 1996; Graves, 2011).\n",
    "* This can be interpreted as a stochastic implementation of <font color=\"red\">Bayesian inference</font> over the weights.\n",
    "* <font color=\"red\">Adding noise to the weights is a practical, stochastic way to reﬂect this uncertainty</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### traditional form of regularization\n",
    "* Noise applied to the weights can also be interpreted as <font color=\"red\">equivalent (under some assumptions) to a more traditional form of regularization</font>, encouraging stability ofthe function to be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color=\"blue\">Consider the regression setting</font>, where we wish to train a function ˆy(x) that maps a set of features x to a scalar using the least-squares cost function between the model predictions ˆy(x) and the true values y:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.5.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now assume that with each input presentation we also include a random perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.5.2.png\" width=200 /> of the network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us imagine that wehave a standard l-layer MLP. We denote the perturbed model as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.5.3.png\" width=100 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Despitethe injection of noise, we are still interested in minimizing the squared error of the output of the network</font>. The objective function thus becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.5.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small $η$, the minimization of $J$ with added weight noise (with covariance $ηI$) is <font color=\"blue\">equivalent to minimization of $J$ with an additional regularization term<font> :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.5.5.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">This form of regularization encourages the parameters to go to regions of parameter space where small perturbations of the weights have a relatively small inﬂuence on the output</font>.\n",
    "* In other words, it pushes the model into regions where the model is relatively insensitive to small variations in the weights, ﬁnding points that are not merely minima, but minima surrounded by ﬂat regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">In the simpliﬁed case of linear regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.5.6.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this <font color=\"red\">regularization term collapses into</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap7.5.7.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is not a function of parameters and therefore does notcontribute to the gradient of ˜JW with respect to the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5.1 Injecting Noise at the Output Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most datasets have some amount of mistakes in they labels.\n",
    "* It can be harmful tomaximizelog $logp(y|x)$ when $y$ is a mistake. \n",
    "* One way to prevent this is to <font color=\"red\">explicitly model the noise on the labels</font>.\n",
    "    - For example, we can assume that for some small constant $\\epsilon$ , \n",
    "        - the training set label $y$ is correct with probability 1−$\\epsilon$, \n",
    "        - and otherwise any of the other possible labels might be correct. \n",
    "        - <font color=\"red\">This assumption is easy to incorporate into the cost function analytically, rather than by explicitly drawing noise samples</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label smoothing  \n",
    "* For example,label smoothing regularizes a model based on a softmax with $k$ output values \n",
    "    - by replacing the hard 0 and 1 classiﬁcation targets \n",
    "        - with targets of $\\epsilon$/$(k−1)$ and 1− $\\epsilon$, respectively. \n",
    "* The standard cross-entropy loss may then be used with these soft targets.\n",
    "* Maximum likelihood learning with a softmax classiﬁer and hard targets <font color=\"red\">may actually never converge</font> \n",
    "    - the softmax can never predict a probability of exactly 0 or exactly 1, \n",
    "    - so it will continue to learn larger and larger weights,\n",
    "    - making more extreme predictions forever.\n",
    "    - <font color=\"blue\">It is possible toprevent this scenario using other regularization strategies like weight decay</font>.\n",
    "* <font color=\"red\">Label smoothing has the advantage of preventing the pursuit of hard probabilities without discouraging correct classiﬁcation</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] DEEP LEARNING (Yoshua Bengio)- http://www.deeplearningbook.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
