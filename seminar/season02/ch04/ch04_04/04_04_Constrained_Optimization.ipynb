{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Numerical Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝 이론 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 4.1 Overflow and Underflow\n",
    "* 4.2 Poor Conditioning\n",
    "* 4.3 Gradient-Based Optimization\n",
    "* <font color=\"red\">4.4 Constrained Optimization</font>\n",
    "* 4.5 Example: Linear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sometimes we wish not only to maximize or minimize a function $f(x)$ over all possible values of $x$. \n",
    "* Instead we may wish to ﬁnd the maximal or minimalvalue of $f(x)$ for values of $x$ in some set $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.bath.ac.uk/mech-eng/constraintmodelling/images/conopt.jpg\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feasible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points $x$ that lie within the set $S$ are called <font color=\"red\">feasible points</font> in constrained optimization terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://flylib.com/books/3/287/1/html/2/images/10fig06.jpg\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### small solution (norm constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We often wish to ﬁnd a solution that is small in some sense. \n",
    "* A commonapproach in such situations is to impose a norm constraint, such as \n",
    "    - $||x|| ≤ 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One simple approach to constrained optimization is simply to <font color=\"red\">modify gradient descent taking the constraint into account</font>.\n",
    "* step size\n",
    "    - If we use a <font color=\"red\">small constant step size</font> \t&#949;, we can make gradient descent steps, then project the result back into $S$.\n",
    "* line search\n",
    "    - If we use a line search, \n",
    "        - we can search only over step sizes &#949; that yield new $x$ points that are feasible, \n",
    "        - or we can project each point on the line back into the constraint region.\n",
    "        <img src=\"http://image.slidesharecdn.com/131110gradientdescentmethod-141128132442-conversion-gate01/95/gradient-descent-method-24-638.jpg?cb=1417181134\" width=600 />\n",
    "        <img src=\"http://image.slidesharecdn.com/131110gradientdescentmethod-141128132442-conversion-gate01/95/gradient-descent-method-31-638.jpg?cb=1417181134\" width=600 />\n",
    "        \n",
    "* tengent space\n",
    "    - When possible, this method can be made more eﬃcient <font color=\"red\">by projecting the gradient into the tangent space of the feasible region</font> before taking the step or beginningthe line search\n",
    "    <img src=\"http://www.frontiersin.org/files/Articles/82010/fphy-02-00019-HTML/image_m/fphy-02-00019-g003.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unconstrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unconstrained optimization ->  constrained optimization problem\n",
    "A more sophisticated approach is to design a diﬀerent, unconstrained optimization problem whose solution can be <font color=\"red\">converted into a solution to the original, constrained optimization problem</font>.\n",
    "* For example, \n",
    "    - if we want to minimize \n",
    "        - $f(x)$ for $x ∈ R^2$ \n",
    "            - with $x$ constrained to have exactly unit $L^2$ norm, \n",
    "    - we can instead minimize \n",
    "        - $g(θ) = f([cos θ, sin θ]^T)$ \n",
    "            - with respect to $θ$, \n",
    "    - then return $[cosθ, sinθ]$ as the solution to the original problem.\n",
    "    \n",
    "This approach requires creativity\n",
    "* the transformation between optimization problems <font color=\"red\">must be designed speciﬁcally for each case we encounter</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Karush–Kuhn–Tucker(KKT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 참고\n",
    "* [2] Lagrange Multipliers and the Karush-Kuhn-Tucker conditions - http://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TheKarush–Kuhn–Tucker(KKT) approach provides a very general solution to constrained optimization. With the KKT approach, we introduce a new function called the <font color=\"red\">generalized Lagrangian or generalized Lagrange function</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/lag.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KKT multipliers & generalized Lagrangian \n",
    "\n",
    "We introduce new variables $λ_i$ and $α_j$ for each constraint, these are called the KKT multipliers. The generalized Lagrangian is then deﬁned as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.4.1.png\" width=600 />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.4.2.png\" width=600 />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform constrained maximization, we can construct the generalized La-grange function of $−f(x)$, which leads to this optimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.4.3.png\" width=600 />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also convert this to a problem with maximization in the outer loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.4.4.png\" width=600 />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The inequality constraints are particularly interesting. \n",
    "* We say that a constraint $h^{(i)}(x)$ is active if $h^{(i)}(x∗) = 0$. \n",
    "* If a constraint is not active, then the solution tothe problem found using that constraint would remain at least a local solution if that constraint were removed. \n",
    "* It is possible that an inactive constraint excludes other solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap4.4.5.png\" width=300 />    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple set of properties describe the optimal points of constrained optimization problems. These properties are called the Karush-Kuhn-Tucker (KKT)conditions.\n",
    "* The gradient of the generalized Lagrangian is zero\n",
    "* All constraints on both $x$ and the KKT multipliers are satisﬁed.\n",
    "* The inequality constraints exhibit “complementary slackness”:\n",
    "    <img src=\"figures/cap4.4.6.png\" width=150 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] 4 Numerical Computation (Deep Learning Book) - http://www.deeplearningbook.org/contents/numerical.html\n",
    "* [2] Lagrange Multipliers and the Karush-Kuhn-Tucker conditions - http://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
