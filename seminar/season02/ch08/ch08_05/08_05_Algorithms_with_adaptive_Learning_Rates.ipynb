{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 OPTIMIZATION FROM TRAINING DEEP MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8.5 Algorithms with Adaptive Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Neural network 연구자들은 learning rate가 설정하기 가장 어려운 hyper parameter라고 인지했다.\n",
    "* 비용은 parameter 공간 안의 몇몇 방향에서는 민감하게 동작할 수 있으며, 또한 다른 방향에서는 민감하지 않게 반응할 수 있다.\n",
    "* momentum algorithm을 통해 이러한 문제를 완화 시킬 수는 있지만, 이는 또 다른 hyper parameter를 발생 시킨다.\n",
    "* delta-bar-delta algorithm (Jacobs, 1988)은 training 동안에 모델의 learning rate를 적용시킬 수 있는 초기의 heuristic한 접근법이었다.\n",
    "* 기본적인 알고리즘은 다음과 같은데 loss를 미분한 값이 동일한 부호일 경우에는 learning rate를 증가시키고, 부호가 바뀌었을 경우에는 learning rate를 감소시킨다.\n",
    "* 물론 해당 룰은 full batch optimization에만 적용될 수 있다.\n",
    "* 최근에는 learning rate를 찾기 위한 다양한 방법들이 제시 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### AdaGrad\n",
    "<img src =\"./figures/fig1.png\" />\n",
    "* loss 의 미분 변화량이 큰 값은 learning rate의 변화를 적게 하고, 미분 변화량이 작은 값은 비교적 learning rate 변화를 크게 한다.\n",
    "* convex한 모델에서는 AdaGrad가 좋은 성능을 발휘할 수 있으나, deep learning에서는 훈련 시작 조기에 learning rate가 지나치게 감소할 수가 있다. \n",
    "* AdaGrad는 좋은 성능을 나타내긴 하지만 모든 deep learning model에 해당하는 것은 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### RMSProp\n",
    "<img src=\"./figures/fig2.png\" />\n",
    "<img src=\"./figures/fig3.png\" />\n",
    "* RMSProp는 gradient accumulation을 exponentially weighted moving average로 변경함으로써, non-convex한 환경에서도 더 좋은 성능을 낼 수 있도록한 AdaGrad의 수정판이다.\n",
    "* AdaGrad는 convex한 구조에 도달하기 전에 learning rate가 너무 작게 감소해버리는 문제가 있다.\n",
    "* 기존의 AdaGrad는 모든 gradient가 누적되기 때문에 점점 learning rate가 감소하는 문제가 있었는데, RMSProp에서는 decay rate p라는 값을 통해 누적된 gradient 값을 수정함으로써 이를 해결할 수 있다.\n",
    "* p가 만약에 0.9일 경우 기존의 0.9를 기억하고 나머지 0.1은 gradient에 업데이트 한다. 오래된 gradient 일수록 점점 값이 감쇠하게 되고 새로운 gradient는 새롭게 업데이트가 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "<img src=\"./figures/fig4.png\" />\n",
    "* Adaptive moments 를 줄여서 나온 것이 Adam이다.\n",
    "* RMSProp와 Momentum 방식을 합친 것 같은 알고리즘.\n",
    "* Momentum 방식과 유사하게 기울기의 지수 평균을 저장하고, RMSProp와 유사하게 기울기의 제곱의 지수 평균을 저장함.\n",
    "* 초기에 s와 r이 0에 편향되어 있는 것을 보상하기 위해서 correct bias 매카니즘을 이용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Optimization Algorithm\n",
    "* Schaul et al. (2014) 많은 양의 최적화 알고리즘을 비교 실험 했다.\n",
    "* adaptive learning rate 알고리즘이 좀 더 강인한 성능을 나타냈지만, 최고의 알고리즘을 찾지는 못했다.\n",
    "* 현재, 가장 많이 사용되는 알고리즘으로는 SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta와 Adam이 있다.\n",
    "<img src=\"./figures/fig5.gif\" />\n",
    "<img src=\"./figures/fig6.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
