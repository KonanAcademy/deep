{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12 Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원 \n",
    "<br><br>\n",
    "* 이번 장에서는 컴퓨터 비전, 음성 인식, 자연어 처리 등의 문제를 풀기 위해 deeplearning을 어떻게 사용하는지를 살펴본다.\n",
    "* 가장 먼저 우리는 Large scale deep learning을 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12.1 Large Scale Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 생물학적으로 각각의 뉴런은 지능적이지 않지만, 이러한 뉴런들이 모이게 되면 지능적인 행동을 할 수가 있다.\n",
    "* 이 말은 즉, 뉴런의 개수가 많아야 한다는 것을 강조한다.\n",
    "* 뉴럴 네트워크의 크기가 가장 중요한 요소이기 때문에 deep learning 에서는 고성능 하드웨어 및 소프트웨어 인프라가 요구된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 12.1.1 Fast CPU Implementations\n",
    "* 과거에는 single cpu를 사용하여 뉴럴 네트워크를 학습하였지만, 요즘에 이러한 접근으로는 불충분하다.\n",
    "* 요즘은 GPU를 사용하거나 많은 수의 CPU를 사용하여 뉴럴 네트워크를 처리하고 있다.\n",
    "* 여기서는 CPU를 효율적으로 사용함으로써 성능향상을 일으킬 수 있는 예제를 설명한다.\n",
    "\n",
    "##### fixed-point arithmetic\n",
    "* Vanhoucke et al. (2011)은 부동 소수점 대신 튜닝된 고정 소수점을 생성함으로써, 기존 보다 3배 빠른 성능을 얻었다.\n",
    "* 하지만 CPU 마다 스펙이 다르기 때문에 부동 소수점 구현이 더 빠를 수도 있다.\n",
    "* 여기서 중요한 것은 numerical computation을 보다 특수화하면 큰 효과를 얻을 수도 있다는 것을 말해준다.\n",
    "\n",
    "##### optimize data structure\n",
    "* 데이터 구조를 최적화 함으로써 캐시 누락을 피하고, vector instruction을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 12.1.2 GPU Implementations\n",
    "##### GPU\n",
    "* 원래 GPU는 그래픽 응용 프로그램 용으로 개발이 되었으며, 비디오 게임은 이러한 GPU의 개발을 촉진시켰다.\n",
    "* 그래픽 카드는 각 픽셀의 색상을 결정하기 위해 병렬로 각 픽셀에서 많은 계산을 수행한다.\n",
    "* GPU 연산은 아래의 특징을 가진다.\n",
    "- 동일한 객체의 각 정점에는 동일한 행렬이 곱해진다.\n",
    "- 어떤 행렬을 곱해야 하는지 결정하기 위해 if문을 쓸 필요가 없다.\n",
    "- 계산은 독립적이기 때문에 병렬화가 쉽다.\n",
    "* 그래픽 카드는 높은 병렬 처리와 높은 메모리를 가지도록 설계되었으며, 기존 CPU에 비해 낮은 clock 속도와 낮은 분기 capability를 가졌다.\n",
    "\n",
    "##### GPU and neural network\n",
    "* 뉴럴 네트워크는 일반적으로 많은 분기 또는 정교한 제어를 필요로 하지 않기 때문에 GPU 하드웨어에 적합하다고 할 수 있다.\n",
    "* 또한 뉴럴 네트워크는 독립적으로 처리할 수 있는 여러 개의 개별 뉴럴들로 나눌 수 있기 때문에 GPU의 병렬성에 적합하다.\n",
    "* steinkrau et al. (2005)는 GPU에 기반한 2 layer fully connected 뉴럴 네트워크를 구현함으로써 기존 CPU의 3배 빠른 성능을 냈다.\n",
    "* 그 이후, Chellapilla et al. (2006)는 동일한 기술을 사용하여 supervised convolutional network의 성능을 향상시켰다.\n",
    "\n",
    "##### advent of general purpose GPUs\n",
    "* General purpose GPU의 등장 이후에 graphic card의 인기가 폭발했으며, 이러한 GP-GPU는 rendering subroutine 뿐만 아니라 산술 코드도 실행할 수 있게 되었다.\n",
    "* 또한 GP-GPU는 상대적으로 편리한 프로그래밍 모델, 대규모 병렬 처리, 높은 메모리 대역폭 등의 장점 제공하며 이제는 neural network의 가장 이상적인 플랫폼으로 자리잡게 되었다. (NVIDIA CUDA)\n",
    "\n",
    "##### GPU 구현시 고려사항\n",
    "* GPU 프로그래밍은 CPU와는 본질적으로 다른데 CPU의 경우는 가능한 cache memory로부터 정보를 읽어오는 것이 좋은 코드인데 반해 GPU의 경우는 쓰기 가능한 메모리는 cache에 위치하지 않으며 이로 인해 같은 값을 한 번 계산하고 다시 메모리로부터 읽는 것보다 한꺼번에 두 번 계산함으로써 더 빠른 계산을 할 수가 있다. \n",
    "* GPU 코드는 본질적으로 멀티 쓰레드이며, 각각의 쓰레드는 조심스럽게 접근되어야 한다. 예를 들어 Coalesced reads or write는 몇몇의 쓰레드가 동시에 하나의 값에 접근하고자 할 때 발생한다. \n",
    "* coalesced memory access는 각 thread가 메모리의 연속된 값에 접근할 때 한꺼번에 가져오거나 쓰는 것을 말함.\n",
    "* 또한 GPU의 각 쓰레드는 동시에 동일한 명령어를 실행하는지를 확인해야 한다. 쓰레드는 warps라고 불리는 작은 그룹으로 나뉘는데, 동일한 warps내에 다른 쓰레드가 다른 코드를 실행해야 하는 경우라면 이런 경우는 병렬이 아니라 순차적으로 처리해야 한다.\n",
    "<br><br>\n",
    "* pylearn2나 Theano, TensorFlow, Torch 같은 경우 새로운 GPU 코드를 작성하지 않아도 CPU 또는 GPU에서 실행될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3 Large Scale Distributed Implementations\n",
    "* 단일 컴퓨터에서의 계산 리소스가 충분하지 않을 수 있기 때문에 분산 처리를 통해 이를 해결할 수 있다.\n",
    "* Bengio et al.(2001)과 Recht et al.(2011)은 asynchronous stochastic gradient descent를 통해 병렬 환경에서 gradient descent를 적용했다.\n",
    "* 핵심은 파라미터 공유인데, 각각의 core는 lock 없이 parameter를 읽은 다음 lock 없이 parameter를 증가시킨다.\n",
    "* 몇몇의 코어가 진행 사항이 겹치기 때문에 gradient descent를 더 빠르게 진행할 수 있다.\n",
    "* Dean et al.(2012)는 매개변수를 공유 메모리에 저장하는 것이 아니라 파라미터 서버라는 따로 관리되는 서버에 둠으로써 멀티 머신을 구현했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.4 Model Compression\n",
    "* 2006년에 모델 압축은 원래의 비싼 모델을 더 적은 메모리와 적은 런타임이 필요한 작은 모델로 바꾸는 것이었다.\n",
    "* 모델 압축은 주로 원본 model의 크기가 overfitting 되는 것을 막기 위한 용도로 적용된다.\n",
    "* 대부분의 경우 앙상블 모델이 가장 낮은 일반화 에러를 내지만, 앙상블은 평가 비용이 비싸다.\n",
    "* 때때로 모델 크기가 크더라도 dropout 등으로 정규화가 잘 되어 있다면 일반화가 잘 된다.\n",
    "* 이러한 모델들을 가지고 몇몇의 f(x)라는 함수를 학습시킨다. 이러한 함수 f(x)에 모델을 맞춘다음에 무작위로 많은 샘플 예제를 훈련세트로 생성한다.\n",
    "* 그런 다음 더 작은 모델을 f(x)와 일치하도록 훈련시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.5 Dynamic Structure\n",
    "* Data processing system은 주어진 입력에 대해 많은 뉴럴 네트워크 중 어떤 네트워크가 실행되어야 하는지를 동적으로 결정한다.\n",
    "* 각각의 네트워크는 입력으로부터 주어진 정보를 계산하기 위해 hidden units을 어떻게 구성할지를 동적으로 실행할 수도 있다.\n",
    "* 뉴럴 네트워크 내의 이러한 동적 구조를 때때로 conditional computation이라고 부르기도 한다.(Bengio, 2013; Bengio et al., 2013b)\n",
    "* Dynamic structure of computation의 가장 단순한 버전은 뉴럴 네트워크의 하위 집합 중 어떤 것이 특정 input에 적용되어야 하는지를 결정하는 것이다.\n",
    "* classifier의 추론을 가속화 하기 위해서는 cascade 전략이 필요하며, 드문 객체나 이벤트를 탐지함으로써 적용될 수가 있다.\n",
    "* classifier의 sequence는 아래와 같이 훈련될 수 있다.\n",
    "- sequence에서의 첫번째 classifier는 낮은 capacity를 가지고 high recall을 가지도록 훈련된다. 다시 말해서 객체가 존재할때, 입력에 대해 잘못 선택하지 않도록 훈련되었다.\n",
    "- 마지막 classifier는 high precision을 가지도록 훈련된다.\n",
    "\n",
    "##### cascade can achive high capacity\n",
    "* 한 가지 방법은 cascade의 나중 맴버를 개별적으로 high capacity를 가지게 하는 방법이다. 이렇게 할 경우 시스템 전체가 high capacity를 가지게 된다.\n",
    "* 다른 방법으로는 각각의 low capacity를 가진 모델들을 조합함으로써 high capacity의 cascade를 만들 수도 있다.\n",
    "* Vioal and Jones(2001)은 digital camera에 적합한 얼굴 검출기를 구현하기 위해 cascade of boosted decision tree를 사용했다.\n",
    "* 지역적으로 슬라이딩 윈도우 방식을 사용해서 얼굴을 지역화 하는데 얼굴이 없는 경우는 해당 input를 reject한다.\n",
    "* Goodfeellow et al(2014d)., Google은 two-step의 cascade를 사용해서 Street View를 주소로 바꾸는데 사용했다. 먼저 하나의 학습 모델을 사용해서 주소를 찾은 다음,다음 학습 모델로 옮겨서 추가로 처리했다. \n",
    "* cascade의 초기 모델에서 객체를 지역화 하고, 나중 모델에서 객체의 위치를 고려한 처리를 진행한 것이다.\n",
    "\n",
    "##### mixture of experts #####\n",
    "* deeplearning 과 dynamic을 조합할 수 있는 가장 단순한 방법은 각 노드가 neural network를 사용해서 decision tree를 훈련시키는 것이다.\n",
    "* 또한 여러 expert network 중 어느 것이 현재 입력에 대해 출력을 계산해야 하는지를 선택하기 위해 gater라 불리는 neural network를 사용할 수도 있다.\n",
    "* 이를 mixture of expert라고 부르며(Nowlan, 1990; Jacobs et al, 1991) gater는 expert의 확률을 출력하고 최종 결과는 expert의 출력의 가중치 조합으로 얻어진다.\n",
    "* 이 전략은 협동이 아니기 때문에 gating 결정의 수가 적을 때 효과적이다. 하지만 만약 다른 하위 집합의 파라미터나 unit을 선택하기를 원할 경우, 모든 gater configuration을 열거해야 하기 때문에 'soft switch'를 사용할 수가 없다.\n",
    "* 이러한 문제를 해결하기 위해 몇 가지 알고리즘이 제시되었다.\n",
    "* Bengio et al. (2013b)는 gating 확률 상에서의 여러 gradient estimator를 가지고 실험했으며, Bacon et al.(2015)와 Bengio et al.(2015a)는 hidden unit 상에서의 조건부 dropout을 학습하기 위해 강화학습을 사용했고, 실제 품질에 영향을 미치지 않으면서 계산 비용을 줄일 수 있는 효과를 봤다.\n",
    "\n",
    "##### attention mechanism #####\n",
    "* 동적 구조의 또다른 종류는 hidden unit이 context에 따라 다른 unit으로부터 입력을 수신할지를 결정하는 switch이다.\n",
    "* 이러한 접근법은 attention mechanism(Olshausen et al., 1993)으로 해석할 수 있다.\n",
    "\n",
    "##### obstacle to using dynamically structured system #####\n",
    "* 다른 입력에 대해 서로 다른 분기를 따르는 시스템 때문에 병렬 처리에 대한 효과가 감소할 수 있다.\n",
    "* 이를 방지하기 위해 특수화된 서브루틴을 작성할 수 있지만 효율적으로 구현하기는 상당히 어렵다.\n",
    "* 예제를 모두 동일한 분기를 사용하는 그룹으로 분할하고, 이러한 그룹의 예제를 동시에 처리함으로써 문제를 완화시킬 수 있다.\n",
    "* 또한 예제를 지속적으로 처리해야 하는 실시간의 경우, 작업 부하에 따른 로드 균형 문제가 발생할 수 있다.\n",
    "* 예를 들어 cascade의 첫번째 모델의 경우 과부하가 걸리고 마지막 모델은 과소 부하가 걸리는 경향이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 12.1.6 Specialized Hardware Implementations of Deep Networks\n",
    "* neural network의 연구 초기 때부터, 하드웨어 설계자들은 neural network 알고리즘의 추론과 학습에 성능을 향상시킬 수 있는 특수한 hardware 구현에 노력했다.\n",
    "* 일반적으로 범용 처리 장치(CPU, GPU) 상에서의 sofrware 구현은 32 또는 64 bit를 사용함으로써 부동 소수점을 나타내지만, 추론시에는 이러한 요소가 정밀도를 떨어뜨린다고 잘 알려져 있다.\n",
    "* 게다가 GPU의 사용이 점점 늘어나면서 이 문제가 더욱 중요하게 대두되고 있다.\n",
    "* 하드웨어의 또 다른 특징으로 점점 단일 CPU,GPU 코어에 대한 연구가 느려지고, 코어를 통한 병렬화 연구가 활발해 지고 있다.\n",
    "* backprop 기반의 neural net 최근 연구에 따르면, 8 bit와 16 bit 사이의 precision은 back-propagation을 이용한 neural network를 훈련하거나 사용하는데 충분하다고 한다.\n",
    "* 명확한 것은 추론시보다 훈련과정에서 더 많은 precision이 필요하며 동적으로 고정 소수점을 사용하여 필요한 bit 수를 줄일 수가 있다.\n",
    "* 부동 소수점이 아닌 고정 소수점을 사용하여, 숫자당 필요한 bit 수를 줄이게 되면, 하드웨어의 표면적, 요구되는 전력, 계산 시간 등을 줄일 수가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
