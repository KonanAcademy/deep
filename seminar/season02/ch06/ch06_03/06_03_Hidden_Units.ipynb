{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Deep Feedfowrd Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Îî•Îü¨Îãù ÏÑ∏ÎØ∏ÎÇò : Ïù¥Î°† [1]\n",
    "* ÍπÄÎ¨¥ÏÑ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 6.1 Example: Learning XOR\n",
    "* 6.2 Gradient-Based Learning\n",
    "* <font color=\"red\">6.3 Hidden Units</font>\n",
    "    - 6.3.1 RectiÔ¨Åed Linear Units and Their Generalizations\n",
    "    - 6.3.2 Logistic Sigmoid and Hyperbolic Tangent\n",
    "    - 6.3.3 Other Hidden Units\n",
    "* 6.4 Architecture Design\n",
    "* 6.5 Back-Propagation and Other DiÔ¨Äerentiation Algorithms\n",
    "* 6.6 Historical Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Hidden Units\n",
    "* 6.3.1 RectiÔ¨Åed Linear Units and Their Generalizations\n",
    "* 6.3.2 Logistic Sigmoid and Hyperbolic Tangent\n",
    "* 6.3.3 Other Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ï∞∏Í≥†\n",
    "* [2] CS231n: Convolutional Neural Networks for Visual Recognition : Training Neural Networks Part 1, activation functions, weight initialization, gradient flow, batch normalization, babysitting the learning process, hyperparameter optimization -   http://cs231n.stanford.edu/slides/winter1516_lecture5.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">how to choose the type of hidden unit to use in the hidden layers of the model.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">The design of hidden units is an extremely active area of research and does not yet have many deÔ¨Ånitive guiding theoretical principles.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">RectiÔ¨Åed linear units are an excellent default choice of hidden unit.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diÔ¨Äerentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Some of the hidden units included in this list are not actually diÔ¨Äerentiable at all input points</font>. \n",
    "* For example, the rectiÔ¨Åed linear function g(z)=max{0, z}is not diÔ¨Äerentiable at z= 0. \n",
    "* This may seem like it invalidates g for use with a gradient-based learning algorithm. \n",
    "* <font color=\"red\">In practice, gradient descent still performs well enough for these models to be used for machine learning tasks</font>.\n",
    "* This is in part because neural network training algorithms do not usually arrive at a local minimum of the cost function, but <font color=\"red\">instead merely reduce its value signiÔ¨Åcantly</font>, as shown in Fig. 4.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig4.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  left derivative & right derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nn.readthedocs.org/en/rtd/image/relu.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hidden units that are not diÔ¨Äerentiable are usually non-diÔ¨Äerentiable at only a small number of points.\n",
    "*  In general, a function g(z) has  \n",
    "    - a left derivative \n",
    "        - deÔ¨Åned by the slope of the function immediately to the left of z and \n",
    "    - a right derivative \n",
    "        - deÔ¨Åned by the slope of the function immediately to the right of z\n",
    "    - A function is diÔ¨Äerentiable at z only if both the left derivative and the right derivative are deÔ¨Åned and equal to each other.\n",
    "    - In the case of g(z) = max{0, z}, the left derivative at z= 0 is 0 and the right derivativeis 1.\n",
    "* The important point is that in practice one can safely disregard the non-diÔ¨Äerentiability of the hidden unit activation functions described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless indicated otherwise, <font color=\"red\">most hidden units</font> can be described as accepting a vector of inputs x, <font color=\"red\">computing an aÔ¨Éne transformation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.54.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then applying an <font color=\"red\">element-wise nonlinear function</font> g(z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 RectiÔ¨Åed Linear Units and Their Generalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nn.readthedocs.org/en/rtd/image/relu.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.55.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RectiÔ¨Åed linear units are easy to optimize because they are so similar to linear units.\n",
    "* first order derivate\n",
    "    - The only diÔ¨Äerence between a linear unit and a rectiÔ¨Åed linear unit is that a rectiÔ¨Åed linear unit outputs zero across half its domain. \n",
    "    - This makes the derivatives through a rectiÔ¨Åed linear unit remain large whenever the unit is active.\n",
    "    - The gradients are not only large but also consistent.\n",
    "* second order derivate\n",
    "    - <font color=\"red\">The second derivative of the rectifying operation is 0 almost everywhere, and the derivative of the rectifying operation is 1 everywhere that the unit is active</font>. \n",
    "    - This means that the gradient direction is far more useful for learning than it would be with activation functions that introduce second-order eÔ¨Äects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RectiÔ¨Åed linear units are typically used on top of an aÔ¨Éne transformation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.56.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">When initializing the parameters</font> of the aÔ¨Éne transformation, it can be a good practice to <font color=\"red\">set all elements of b to a small, positive value, such as 0.1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generalizations of rectiÔ¨Åed linear units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One drawback to rectiÔ¨Åed linear units is that they cannot learn via gradient-based methods on examples for which their activation is zero. \n",
    "* A variety of generalizations of rectiÔ¨Åed linear units guarantee that they receive gradient everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three generalizations of rectified linear units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three generalizations of ReLU are based on using a non-zero slope $Œ±_i$ when $z_i$ < 0: $h_i$ = $g(z,Œ±)_i$ = max(0,$z_i$) + $Œ±_i$min(0,$z_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Absolute value rectiÔ¨Åcation : $ùõº_ùëñ$ = -1 to obtain g(z) = |z|.  \n",
    "    <img src=\"http://i.stack.imgur.com/P3xT5.png\" width=300 />\n",
    "* leaky ReLU & parametric ReLU (PReLU)\n",
    "    - Leaky ReLU(Maas et al., 2013) fixes $ùõº_ùëñ$ to a small value like 0.01 while a parametric ReLU(PReLU) treats $ùõº_ùëñ$ as a learnable parameter\n",
    "    <img src=\"http://gforge.se/wp-content/uploads/2015/05/PReLU.jpg\" width=400 />\n",
    "* Maxout units\n",
    "    <img src=\"http://fastml.com/images/pylearn2/digits/maxout.png\"  width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute valu rectification, leaky ReLU, parametric ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ï∞∏Í≥†\n",
    "* [3] Benchmarking ReLU and PReLU using MNIST and Theano - http://gforge.se/2015/06/benchmarking-relu-and-prelu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.57.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maxout units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ï∞∏Í≥†\n",
    "* [4] Maxing out the digits - http://fastml.com/maxing-out-the-digits/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxout units (Goodfellow et al., 2013a) generalize rectiÔ¨Åed linear units further. Instead of applying an element-wise functionn g(z), maxout units divide z into groups of k values. Each maxout unit then outputs the maximum element of one of these groups:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.58.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maxout : learning the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A maxout unit can learn a piecewise linear, convex function with up tokpieces.\n",
    "* <font color=\"red\">Maxout units can thus be seen as learning the activation function</font> itself rather than just the relationship between units. \n",
    "* <font color=\"red\">With large enough k, a maxout unit can learn to approximate any convex function with arbitrary Ô¨Ådelity</font>.\n",
    "    -  In particular, a maxout layer with two pieces can learn to implement the same function of the input x as a traditional layer using \n",
    "        - the rectiÔ¨Åed linear activation function,\n",
    "        - absolutevalue rectiÔ¨Åcation function, or \n",
    "        - the leaky or parametric ReLU, or \n",
    "        - can learn to implement a totally diÔ¨Äerent function altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maxout : regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each maxout unit is now parametrized by k weight vectors instead of just one, so maxout units typically need more regularization than rectiÔ¨Åed linear units. \n",
    "* They can work well without regularization if the training set is large and the number of pieces per unit is kept low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### maxout : other benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maxout units have a few other beneÔ¨Åts. \n",
    "* In some cases, one can gain some statistical and computational advantages by <font color=\"red\">requiring fewer parameters</font>.\n",
    "     - SpeciÔ¨Åcally, if the features captured by n diÔ¨Äerent linear Ô¨Ålters can be summarized without losing information by taking the max over each group of k features, then the next layer can get by with k times fewer weights.\n",
    "* Because each unit is driven by multiple Ô¨Ålters, maxout units have some redundancy that helps them to <font color=\"red\">resist a phenomenon called catastrophic forgetting</font> in which neural networks forget how to perform tasks that they were trained on in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> RectiÔ¨Åed linear units and all of these generalizations of them are based on the principle that models are easier to optimize if their behavior is closer to linear</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Logistic Sigmoid and Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.byclb.com/TR/Tutorials/neural_networks/ch7_1_dosyalar/image028.jpg\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to the introduction of rectiÔ¨Åed linear units, most neural networks used the <font color=\"red\">logistic sigmoid activation function</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.59.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the <font color=\"red\">hyperbolic tangent activation function</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.60.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These activation functions are closely related because"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.61.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sigmoid units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We have already seen sigmoid units as output units, used to predict the probability that a binary variable is 1\n",
    "* Unlike piecewise linear units, sigmoidal units saturate across most of their domain.\n",
    "* The widespread saturation of sigmoidal units can make gradient-based learning very diÔ¨Écult.\n",
    "* <font color=\"red\">For this reason,their use as hidden units in feedforward networks is now discouraged.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyperbolic tangent activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid.\n",
    "* This makes training the tanh network easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoidal activation functions are more common in settings other than feed-forward networks. \n",
    "* <font color=\"red\">Recurrent networks, many probabilistic models, and some autoencoders have additional requirements that rule out the use of piecewiselinear activation functions and make sigmoidal units more appealing despite th edrawbacks of saturation.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.62.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 Other Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other types of hidden units are possible, but are used less frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, <font color=\"red\">a wide variety of diÔ¨Äerentiable functions perform perfectly well</font>.\n",
    "* Many unpublished activation functions perform just as well as the popular ones.\n",
    "* To provide a concrete example, the authors tested a feedforward network using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.63.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on the MNIST dataset and obtained an error rate of less than 1%, which is competitive with results obtained using more conventional activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During research and development of new techniques, it is common to test many diÔ¨Äerent activation functions and Ô¨Ånd that several variations on standard practice perform comparably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be impractical to list all of the hidden unit types that have appearedin the literature. <font color=\"red\">We highlight a few especially useful and distinctive ones</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.64.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.65.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax units\n",
    "\n",
    "* Softmax units are another kind of unit \n",
    "    - that is usually used <font color=\"red\">as an output</font> \n",
    "    - but may sometimes be used <font color=\"red\">as a hidden unit</font>.\n",
    "* Softmax units naturally \n",
    "    - represent a <font color=\"red\">probability distribution</font> \n",
    "        - over a discrete variable with <font color=\"red\">k possible values</font>, \n",
    "    - so they may be used <font color=\"red\">as a kind of switch</font>.\n",
    "* These kinds of hidden units are usually only used in \n",
    "    - more advanced architectures that \n",
    "        - <font color=\"red\">explicitly learn to manipulate memory</font>, \n",
    "            - described in section 10.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few other reasonably common hidden unit types include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ï∞∏Í≥†\n",
    "* [5] Comprehensive list of activation functions in neural networks with pros/cons - http://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial basis function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ï∞∏Í≥† \n",
    "* [6] RBF Neural Networks - https://www.dtreg.com/solution/view/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dtreg.com/uploaded/pageimg/PnnRbf.jpg\" width=600 />\n",
    "<img src=\"https://www.dtreg.com/uploaded/pageimg/PnnRadialBasisFunction.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.66.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This function becomes more active as $x$ approaches a template W:,i.\n",
    "* Because itsaturates to 0 for most x, <font color=\"red\">it can be diffcult to optimize</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f7/Rectifier_and_softplus_functions.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.67.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is a smooth version of the rectiÔ¨Åer,introduced by Dugas et al. (2001) for function approximation and by Nairand Hinton (2010) for the conditional distributions of undirected probabilistic models.\n",
    "* Glorot et al. (2011a) compared the softplus and rectiÔ¨Åer and found better results with the latter. \n",
    "* <font color=\"red\">The use of the softplus is generally discouraged.</font>\n",
    "* The softplus demonstrates that the performance of hidden unit types can be very counterintuitive \n",
    "    - one might expect it to have an advantage overthe rectiÔ¨Åer due to being differentiable everywhere or due to saturating less completely, but empirically it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.stack.imgur.com/CJnMI.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.68.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this is shaped similarly to thetanhand the rectiÔ¨Åer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Hidden unit design remains an active area of research and many useful hidden unit types remain to be discovered.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ï∞∏Í≥†ÏûêÎ£å\n",
    "* [1] DEEP LEARNING (Yoshua Bengio)- http://www.deeplearningbook.org/\n",
    "* [2] CS231n: Convolutional Neural Networks for Visual Recognition : Training Neural Networks Part 1, activation functions, weight initialization, gradient flow, batch normalization, babysitting the learning process, hyperparameter optimization -   http://cs231n.stanford.edu/slides/winter1516_lecture5.pdf\n",
    "* [3] Benchmarking ReLU and PReLU using MNIST and Theano - http://gforge.se/2015/06/benchmarking-relu-and-prelu/\n",
    "* [4] Maxing out the digits - http://fastml.com/maxing-out-the-digits/\n",
    "* [5] Comprehensive list of activation functions in neural networks with pros/cons - http://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons\n",
    "* [6] RBF Neural Networks - https://www.dtreg.com/solution/view/25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
