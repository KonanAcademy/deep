{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5.10 Building a Machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 거의 모든 딥러닝 알고리즘은 dataset, cost funciton, 최적화 함수 그리고 모델의 결합으로 설명될 수 있다.\n",
    "* 예를 들어 선형 회귀 알고리즘의 경우, dataset은 x,y 그리고 아래의 cost function과 모델, 그리고 최적화 알고리즘은 cost가 0이 되는 gradient를 풀어감으로써 설명할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.1.png\" width=300 />\n",
    "<img src=\"figures/fig5.2.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 또한 cost function과 모델, 최적화 알고리즘 등을 바꿔가면서 다양한 딥러닝 알고리즘을 얻을 수 있다.\n",
    "* 주로 대부분 cost function은 negative log-likelihood를 사용한다. cost 함수의 최소값이 maximum likelihood 측정 값이기 때문이다.\n",
    "* cost function은 regularization term을 추가할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.3.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 만약에 모델이 nonlinear로 변한다면, 대부분의 cost function은 closed form에서는 최적화 하지 못할 것이다.\n",
    "* gradient descent 등과 같은 반복적인 수치 최적을 통해 해결해야 할 것이다.\n",
    "* model, costs, optimization의 결합을 통한 학습 알고리즘은 supervised와 unsupervised 학습에도 적용이 된다.\n",
    "* supervised에서는 선형 회귀가 대표적인 예이고, unsupervised에서는 오직 X 값만 포함된 데이터셋과 cost, model에 의해서 적용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig5.4.png\" width=300 />\n",
    "<img src=\"figures/fig5.5.png\" width=150 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* decision tree나 k-means 등의 몇몇 모델들은 특별한 optimizer를 사용한다. 왜냐하면 cost function이 flat region을 가지고 있기 때문에 gradient optimizer로는 찾아 낼 수가 없기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
