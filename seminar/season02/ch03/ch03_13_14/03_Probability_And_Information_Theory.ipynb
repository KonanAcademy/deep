{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Probability And Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3.13 Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 하나의 신호 안에 얼마나 많은 정보가 나타나는지를 정량적으로 나타내는 응용 수학의 한 분야\n",
    "* 라디오 전파 같은 noisy 채널 상으로 보내진 메시지를 연구하는 데에서 시작.\n",
    "* Information Theory를 통해\n",
    "  - 어떻게 최적의 코드를 설계할 것인가?\n",
    "  - 특정 확률 분포로부터 샘플된 메시지의 길이를 어떻게 계산할 것인가?\n",
    "* machine learning에서는 continuous variable에도 information theory를 적용해 볼 수 있다. ex) 어떤 범위내에서 연속적 실수 값을 가지는 형태. 몸무게, 길이\n",
    "* information Theory의 가장 기본적인 사실은 자주 발생하지 않을 이벤트가 더 많은 정보량을 제공한다는 것\n",
    "  - ex) 오늘 아침에 해가 떳다. 오늘 아침에 일식이 있었다.\n",
    "* information theory의 세 가지 속성.\n",
    "  - 자주 일어나는 이벤트는 낮은 정보량을 가져야 한다. 반드시 일어날 이벤트는 정보량을 가져서는 안된다.\n",
    "  - 드물게 일어나는 이벤트는 높은 정보량을 가져야 한다.\n",
    "  - 독립적인 이벤트는 추가적인 정보를 가져야 한다. 예를 들어 동전을 던져서 앞면이 나올 확률이 2번 발견되었다는 것은, 1번 발견된 사실보다 2배 많은 정보량을 전달해야 한다.\n",
    "* 위의 3가지 속성을 만족시키기 위해서 event x가 x가 되는 self-information을 정의하고 아래의 공식을 사용한다. (정보의 양이 확률에 반비례함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.1.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 책에서는 자연로그를 사용한다. 그래서 정보량 결과 단위는 nats 이다.\n",
    "* 1 nat = 1/e 확률.\n",
    "* 2를 밑으로 하는 로그일 경우 단위는 bits 또는 shannons을 사용한다.\n",
    "* self-information은 오직 하나의 결과만을 처리하지만, Shannon entropy를 사용함으로써 전체 확률 분포에서 불확실에 대한 합들을 정량화 할 수가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.2.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shannon entropy의 결과 값은 기대되는 정보의 합."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 확률이 확실시 되는 경우에는 낮은 entropy를 가지며, 확률이 균등하게 분포되어 있는 경우에는 높은 entropy를 가진다.\n",
    "* 만약 x 값이 연속적이면, Shannon entropy를 differential entropy라고 부른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.70043971814\n",
      "0.235193381819\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def entropy(s):\n",
    "    p, lns = Counter(s), float(len(s))\n",
    "    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())\n",
    "\n",
    "print entropy(\"abcdefghijklmnopqrstuvwxyz\");\n",
    "print entropy(\"eeeeeeeeeeeeeeeeeeeeeeeeez\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 동일한 랜덤 변수 x에 대해서 두 개의 확률 분포 P(x)와 Q(x)로 분리했을 경우, KL(Kullback-Leibler) devergence를 사용함으로써 두 개의 분포 사이의 차이를 측정할 수가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.4.png\" width=600 />\n",
    "<img src=\"figures/fig3.6.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KL devergence는 음수가 아니다.\n",
    "* 만약 P와 Q가 같은 분포를 나타낸다면(이산 변수), 0 값을 가진다.\n",
    "* 확률 분포 Q를 P 대신 사용할 경우의 엔트로피의 변화를 나타냄.\n",
    "* 또한 비대칭이기 때문에 실제로 거리를 구하는 측정 방법은 아니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.5.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.7.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 3.14 Structured Probabilistic Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 단일 함수로 확률 분포를 표현하기 보다는, 확률 분포를 여러 요소들로 나누는게 더 효율적이다. (computationally, statistically)\n",
    "* a,b,c라는 세 개의 랜덤 변수가 있으며, a는 b에 영향을 주는 요소이고, b는 c에 영향을 주는 요소이지만 a와 c는 독립적이라고 할 경우 아래와 같이 표현 할 수가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.8.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 이러한 factorization은 분포를 설명하기 위해 필요한 파라미터의 수를 크게 줄일 수 있다.\n",
    "* 또한 그래프 이론을 사용하여 factorization을 표현할 수 있으며, 이를 structured probabilistic model 또는 graphical model이라고 부른다.\n",
    "* structured probabilistic model에는 directed와 undirected 두가지 종류가 있다.\n",
    "* 두 가지 종류 모두 그래프 내에 각각의 노드들이 있으며 각각의 노드들은 랜덤 변수를 의미한다.\n",
    "* edge가 두개의 랜덤 변수를 연결하고 있다는 것은 확률 분포가 두 개의 랜덤 변수 사이에서 직접적인 상호작용을 표현할 수 있다는 것을 의미한다.\n",
    "* directed model은 방향이 있는 edge를 사용하며, 조건부 확률 분포 내에서 factorization을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.9.png\" width=300 />\n",
    "<img src=\"figures/fig3.10.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Undirected model은 방향이 없는 edge를 사용하며, 함수들의 집합(확률 분포가 아님) 내에서 factorization을 표현한다.\n",
    "* 그래프 내에 서로 연결된 각각의 노드들의 집합을 clique라고 부르며, 각각의 clique는 factor Φ(i)(C(i))로써 사용된다.\n",
    "* 각각의 factor의 출력은 반드시 음수가 아니어야하며, factor들을 합친 결과가 반드시 1이 되어야 한다는 제약은 없다.\n",
    "* 정규화된 확률 분포를 얻기 위해서 1/Z로 나눔.\n",
    "* PART 3에서 좀더 자세히.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig3.11.png\" width=300 />\n",
    "<img src=\"figures/fig3.12.png\" width=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
