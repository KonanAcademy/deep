{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 Linear Factor Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원 \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13.4 Sparse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sparse coding은 linear factor model로써, unsupervised feature learning 과 feature extraction mechanism 등의 분야에서 크게 연구되고 있다.\n",
    "* dictionary는 신호의 원형의 집합이라고 할 수 있는데 모든 x는 dictionary와 sparse 값을 갖는 sparse 벡터의 곱에 의해 표현이 된다. 이 때, sparse 벡터는 다음의 식을 만족하도록 구해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fig1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* D는 dictionary, a는 sparse 벡터, y는 잡음이 있는 데이터, μ는 비중 조절 상수(regularization?)\n",
    "* 잡음이 없는 데이터가 x ≈ Dα와 같이 sparse하게 표현될 수 있다는 가정 하에, 잡음이 있는 데이터와 잡음이 없는 데이터의 차이를 최소화 하고, sparse 벡터의 0이 아닌 성분을 의미하는 1-Norm을 최소화하는 sparse vector를 구하는 것을 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다른 대부분의 linear factor model과 마찬가지로, x를 재생성하기 위해 아래 표와 같이 linear decoder에 noise를 추가시킨다.\n",
    "* 보다 상세하게 sparse coding model은 linear factor가 isotropic precision β와 함께 가우시안 노이즈를 가진다고 가정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/fig2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* p(h)의 분포는 0에 가까운 peak가 선택이 되며, 일반적으로 factorized laplace, cauchy 또는 factorized student-t 분포를 포함한다.\n",
    "* 예를 들어 sparsity penalty 계수 λ의 관점에서 Laplace prior는 아래와 같이 주어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/fig3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student-t prior는 아래와 같이 주어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/fig4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* maximum likelihood를 가진 sparse coding을 training 하는것은 어렵다.\n",
    "* 대신에 주어진 데이터를 더 잘 구성하기 위해 데이터를 인코딩하고 decoder를 훈련하는 것을 번갈아 training 할 수 있다.\n",
    "* PCA의 경우 h를 예측하고, weight 행렬의 곱으로만 구성이 되어 있는 parametric encoder를 사용해왔다.\n",
    "* 하지만 sparse coding에서는 parametric encoder를 사용하지 않으며, 최적화 알고리즘으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./figures/fig5.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Laplace와 Normal distribution이 결합되면 아래의 최적화 문제가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/fig6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 추론을 수행하는 것 보다 모델을 train 시키기 위해서, 우리는 h에 대한 최소화와 w에 대한 최소화를 반복하며, β를 하이퍼 파라미터로 처리한다.\n",
    "* 일반적으로 1로 설정하는데, 최적화 문제에서 λ와 역할이 공유되고, 두개의 하이퍼 파라미터는 필요가 없기 때문이다.\n",
    "* 원칙적으로 β를 모델의 파라미터로 처리하고 학습하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sparse coding의 모든 접근법이 명확하게 p(h)와 p(x|h)를 생성하는 것은 아니다.\n",
    "* 추론시에 종종 activation 값이 0이 되는 특징 값들을 학습할 경우를 살펴보자.\n",
    "* Laplace prior를 통해 h를 샘플링했다면 실제 h의 요소가 0이 될 확률은 0이다.\n",
    "* 또한 generative model 그 자체는 sparse하지 않으며 오직 feature extractor만이 sparse하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Goodfellow(2013d)는 실제 0이 포함된 샘플들을 slab과 sparse coding model에서 추론한 결과를 설명하고 있다.\n",
    "* non parametric encoder의 사용과 결합된 sparse coding 기법은 특정 parametric encoder보다 reconstruction error와 log-prior의 결합을 최소화 할 수 있다.\n",
    "* 또 다른 장점으로 일반화의 오류가 없다.\n",
    "* training data와 유사하지 않은 x의 경우, 훈련된 parametric encoder는 sparse code의 결과인 h를 찾는데 실패할 수가 있다.\n",
    "* inference problem이 convex한 대부분의 sparse coding model은 optimization procedure가 항상 optimal code를 찾아낸다.\n",
    "* cost가 상승할 수 있지만 이는 encoder의 일반화 에러가 아니라 decoder 가중치의 일반화 에러 때문이다.\n",
    "* sparse coding의 encoding process에서 일반화 에러가 발생하지 않기 때문에, parametric function을 사용해서 code를 예측하는 것보다 feature extractor를 사용하면 더 일반화가 잘 될 수 있다.\n",
    "* Coates and Ng(2011)은 sparse coding feature가 parametric encoder인 linear sigmoid autoencoder 기반의 모델보다 객체 인식에 더 일반적이라는 것을 입증했다.\n",
    "* Goodfellow et al.(2013d)는 sparse coding의 variant가 class당 20개 정도로 작은 label을 가진 feature extractor보다 더 우수하다는 것을 보여줬다.\n",
    "* non parametric encoder의 단점은 반복 알고리즘을 실행해야 하기 때문에 주어진 x에서 h를 계산하는데 많은 시간이 요구된다.\n",
    "* 14장에서 설명될 parametric autoencoder 접근법은 오직 고정된 수의 layer만을 사용하며 대개 1개의 layer만을 사용한다.\n",
    "* 또 다른 단점으로 back-propagate가 어렵다.\n",
    "* 대략적인 derivative를 이용한 sparse coding의 수정 버전이 있지만 많이 사용되지 않는다. (Bagnell and Bradley, 2009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/fig7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위의 그림과 같이 sparse coding은 다른 linear factor model과 같이 빈약한 sample을 생성한다.\n",
    "* 이유는 각각의 feature는 잘 학습될 수 있지만, hidden code 상의 factorial prior는 각각의 생성된 sample에서의 모든 특징의 random subset을 포함하는 모델을 생성하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.navisphere.net/4383/ufldl-tutorial-18-unsupervised-learning-sparse-coding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
