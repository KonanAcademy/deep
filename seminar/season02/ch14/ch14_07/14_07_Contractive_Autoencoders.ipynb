{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원 \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14.7 Contractive Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* contractive autoencoder는 h = f(x)에서 f의 미분계수를 가능한한 작게 함으로써 명시적으로 regularizer를 나타낸다.\n",
    "<img src =\"./figures/fig1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ω(h) penalty는 encoder 함수와 관련된 Jacobian matrix의 Frobenius norm(요소의 제곱의 합)의 제곱으로 나타낸다.\n",
    "\n",
    "##### connection between the denoising autoencoder and the contractive autoencoder\n",
    "* Alain과 Bengio(2013)는 x가 r = g(f(x))로 매핑되는 reconstruction 함수 상의 contractive penalty가 denoising reconstruction error와 동일함을 증명했다.\n",
    "* 이 말은 denoising autuencoder는 reconstruction 함수가 작지만 유한한 크기의 입력 변화에 강인하도록 하는데 반해, contractive autoencoder는 feature extraction 함수가 극소의 입력 변화에 강인하도록 만든다.\n",
    "<br><br><br>\n",
    "\n",
    "##### contractive penalty\n",
    "* feature f(x)를 pretrain하기 위해 jacobian 기반의 contractive penalty를 사용할 경우에, g(f(x)) 보다는 f(x)에 contractive penalty를 적용시킨 결과가 분류에 있어 더 정확하게 나타난다.\n",
    "* f(x) 상의 contractive penalty는 14.5.1에서 언급한 것처럼 score matching과도 밀접한 연관을 가지고 있다.\n",
    "* CAE는 입력의 변화에 대해 강인하도록 훈련이 되었기 때문에, input의 인접 점들을 더 작은 출력의 인접점으로 매핑을 해야 한다.\n",
    "* CAE는 오직 지역적으로만 contractive 한다. 즉 training x의 모든 변화는 f(x)와 가깝게 매핑된다.\n",
    "* globally 하게 두 개의 서로 다른 좌표 x와 x1은 원래 좌표보다 멀리 떨어져 있는 f(x)와 f(x1)에 매핑된다.\n",
    "* 또한 f는 data manifold 사이 혹은 data manifold로부터 멀리 떨어진 확장이 가능하다.(그림 14.7 참조)\n",
    "* Ω(h) penalty에 sigmoidal unit이 적용이 될 때, jacobian을 축소하는 가장 쉬운 방법으로는 sigmoid unit을 0 또는 1로 saturate 시키는 것이다.\n",
    "* 이렇게 되면 CAE가 binary code로 해석될 수 있는 sigmoid의 극 값과 함께 input을 encoding 할 수 있다.\n",
    "\n",
    "##### contractive\n",
    "* point X에서의 Jacobian matrix J를 nonlinear encode f(x)를 근사하는 것으로 생각할 수가 있다. \n",
    "* 이것을 보다 공식적으로 \"contractive\"란 단어로 사용할 수가 있다.\n",
    "\n",
    "##### theory of linear operators\n",
    "* linear operator 이론상으로, 만약에 Jx의 norm이 모든 unit-norm x에 대해 1보다 작거나 같다면 linear operator는 contractive된다고 말할 수 있다. \n",
    "* 다시 말해서 만약에 unit sphere가 수축이 된다면 J는 contractive 된다.\n",
    "* 각각의 local linear operator가 contraction이 되도록 하기 위해 cae가 각각의 점 x에서 f(x)의 local linear 근사의 Frobenius norm을 penalizing 한다고 생각 할 수 있다.\n",
    "<br><br>\n",
    "\n",
    "##### reconstruction error and contractive penalty\n",
    "* sec14.6에서는 regularized autoencoder가 두 개의 반대되는 force를 유지함으로써 manifold를 학습하는 것을 살펴봤다.\n",
    "* CAE의 경우엔, 이러한 두 개의 force는 reconstruction error와 contractive penalty로 설명할 수가 있다.\n",
    "* reconstruction error 만으로도 CAE가 indentity function을 학습하는데 도움이 된다.\n",
    "* contractive penalty 만으로도 CAE가 x와 관련된 특징을 학습할 수가 있다.\n",
    "* 두 개의 조합을 통해 autoencoder를 생성한다.\n",
    "<br><br>\n",
    "\n",
    "##### tangent directions\n",
    "* CAE의 목표는 데이터의 manifold 구조를 학습하는 것이다.\n",
    "* 큰 Jx를 가지는 x방향은 h를 빠르게 변화시키므로, manifold의 접평면에 가까운 방향이 된다.\n",
    "* Rifai는 CAE를 훈련함으로써 J의 대부분의 singular value 크기가 1 아래로 떨어지고 그래서 contractive가 되었음을 실험을 통해 보여줬다.\n",
    "* 하지만 몇몇의 singualr value는 1 이상의 값을 가지는데 이유는 reconstruction error penalty가 cae로 하여금 큰 local variance를 가지는 direction을 인코딩하도록 하기 때문이다.\n",
    "* 가장 큰 singular value와 일치하는 방향은 contractive autoencoder가 학습한 접선 방향으로 해석된다.\n",
    "* 이상적으로, 이러한 접선 방향은 data의 실제 변화와 일치해야 한다.\n",
    "* 예를 들어 이미지에 적용된 CAE는 그림 14.6과 같이 이미지의 오브젝트가 점차 포즈를 변경함에 따라 이미지가 어떻게 변하는지를 보여주는 접선 벡터를 학습해야 한다.\n",
    "* 그림 14.10을 보면 sigular vector의 시각화를 함으로써 input 이미지의 의미지는 변환과 일치함을 알 수 있다.\n",
    "<img src=\"./figures/fig2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CAE practice issue\n",
    "* 하나의 hidden layer autoencoder의 경우 계산하는 비용이 싸지만, 더 깊게 autoencoder를 쌓을 경우 계산 비용이 비싸진다.\n",
    "* Rifai는 이전에 autoencoder의 hidden layer를 재구성하도록 훈련된 일련의 단일 layer autoencoder들을 개별적으로 학습했다. \n",
    "* 이러한 autoencoder의 구성은 depp autoencoder를 형성한다.\n",
    "* 각각의 layer는 지역적으로 contractive 하도록 훈련되었기 때문에 deep autoencoder 또한 contractive한다.\n",
    "* 결과는 deep model의 Jacobian 상에서의 penalty를 가진 전체 구조를 훈련한 결과와 같지는 않지만 상당 부분의 특징들을 포착할 수 있다.\n",
    "* 또 다른 문제로 만약에 decoder 상에서 어떤 종료의 scale을 적용하지 않는다면 contraction penalty는 쓸모 없는 결과를 얻을 수가 있다.\n",
    "* 예를 들어 encoder는 작은 상수 ε를 input에 곱함으로써 구성될 수 있고, decoder는 코드를 ε로 나눔으로써 구성할 수 있다.\n",
    "* ε가 0에 가까워짐에 따라 encoder는 contractive penalty Ω(h)가 0에 접근하도록 할 것이다. 하지만 encoder는 분포에 대해 아무것도 학습하지 않았다.\n",
    "* 반면에 decoder는 완변한 reconstruction을 유지한다.\n",
    "* Rifai는 이러한 것을 막기 위해 f와 g의 weight를 사용했다.\n",
    "* f와 g는 affine 변환과 요소별 비선형으로 구성된 표준 neural network이며, 그래서 g의 weight matrix를 f의 weight matrix의 전치 행렬로 설정하는 것이 간단하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
