{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17 Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원 \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17.2 Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/fig1.png\" />\n",
    "* 우리는 위의 식에서 Importance sampling을 이용해 해를 좀 더 효율적으로 구할 수가 있다.\n",
    "* 즉, 근사 적분을 수행해야 할 때, 최대한 적은 샘플링으로 근사값을 구하기 위해 샘플값이 큰 부분을 더 높은 빈도로 샘플링을 한다.<br/>\n",
    "<img src=\"./figures/fig2.gif\" />\n",
    "<img src=\"./figures/fig3.png\" />\n",
    "<img src=\"./figures/fig4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* q의 선택에 따라 importance sampling의 분산이 매우 민감하게 작동할 수 있다.\n",
    "<img src=\"./figures/fig5.png\" />\n",
    "* q가 아래와 같을 때 분산이 최소값이 된다.\n",
    "<img src=\"./figures/fig6.png\" />\n",
    "* Z는 정규화 상수이며, q*(x)의 합이 1이 될 수 있도록 동작된다.\n",
    "* 더 중요한 샘플링 분포에서는 더 많은 가중치가 부여된다.\n",
    "* f(x)의 부호가 변화되지 않는다면, Var[^sq*] = 0 이 될 것이다. 이 의미는 최적의 분포가 사용될 때, 단일 샘플로도 충분하다는 것을 나타낸다.\n",
    "* 물론 이 방법은 q*의 계산이 본래의 문제를 해결할 수 있기 때문에 가능한 일이며, 일반적으로는 최적 분포에서 단일 샘플을 이용한 접근은 실용적이지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### biased importance sampling<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 또 다른 접근법은 biased importance sampling을 사용하는 방법인데, 정규화된 p나 q를 요구하지 않는다는 이점이 있다.\n",
    "* 이산 변수의 경우, biased importance sampling은 아래와 같이 주어진다.<br />\n",
    "<img src=\"./figures/fig7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* p~와 q~는 p와 q의 정규화되지 않은 형태이고, x(i)는 q로부터 나온 sample이다.\n",
    "* n이 ∞일때와 위의 식의 분모가 1로 수렴할 때를 제외하고는 E[^sBIS]는 s와 같지 않기 때문에 편향된다.\n",
    "* 그래서 이러한 추정기를 asymptotically unbiased라고 부른다.\n",
    "* q를 선택하는 게 몬테카를로 추정의 효율성을 크게 향상 시킬수는 있지만, 잘못된 선택을 하면 효율성이 훨씬 떨어질 수가 있다.\n",
    "* p(x)|f(x)| / q(x) 가 큰 q의 샘플이 있다면 분산도 매우 커질수가 있다.\n",
    "* 이런 경우는 q(x)가 작은 반면에 p(x)와 f(x)가 작지 않을 경우에 발생할 수가 있다.\n",
    "* q 분포는 일반적으로 샘플링이 쉽도록 매우 간단한 분포로 선택된다.하지만 x가 고차원일 경우에 q의 단순성은 p 또는 p|f|를 불량하게 매칭시킨다.\n",
    "* q(x) >> p(x)|f(x)| 일 때, importance sampling은 의미없는 샘플들(아주 작은 수 혹은 0 들의 합)을 수집한다.\n",
    "* 반면에 q(x) << p(x)|f(x)|일 경우, 비율은 엄청나게 커질 수 있지만 거의 발생할 확률이 없다.\n",
    "* 후자의 경우 거의 발생하지 않기 때문에, 전형적인 표본에서는 나타나지 않을 수 있으며, 과대 추정으로는 거의 보상되지 않는 s의 과소 추정을 산출한다.\n",
    "* 이러한 매우 크거나 작은 수는 x가 고차원일 때 나타나는데, 높은 차원에서의 joint 확률의 동적 범위가 매우 커질 수 있기 때문이다.\n",
    "* 이러한 위험에도 불구하고 importance sampling 및 그 변형은 딥러닝을 포함한 많은 기계 학습에서 유용하다.\n",
    "* 예를 들어, 많은 수의 어휘를 가진 neural language mode 또는 많은 출력을 내는 neural net에서의 training 가속을 위해 importance sampling을 사용해라.\n",
    "* 또한 partition function을 추정하기 위해 importance sampling을 어떻게 사용하는지도 살펴봐라(sec.18.7)\n",
    "* variational autoencoder와 같은 deep directed model에서의 log-likelihood를 추정하기 위해서 importance sampling을 어떻게 사용하는지도 살펴봐라.(sec 20.10.3)\n",
    "* importance sampling은 cost function의 gradient의 추정을 향상시키기 위해서도 사용될 수 있다.\n",
    "* 더 어려운 예제를 더 빈번하게 샘플링하면 gradient의 분산을 줄일 수가 있다.(Hinton, 2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
