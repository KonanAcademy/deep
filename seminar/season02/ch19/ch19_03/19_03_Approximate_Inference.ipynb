{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19 Approximate inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유주원 \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19.3 MAP Inference and Sparse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 잠재적 변수를 가진 확률 모델을 훈련할 때, 우리는 대개 p(h | v)를 계산한다.\n",
    "* 추론의 또 다른 형태는 가능한 값에 대한 전체 분포를 추론하기 보다는 누락된 변수의 단일 가능성이 가장 높은 값을 계산한다.\n",
    "* 잠재 변수 모델의 맥락에서 이것은 computing을 의미한다.\n",
    "<img src=\"./figures/fig1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이것은 MAP 추론이라고 하는 사후 추론으로 알려져 있다. \n",
    "* MAP 추론은 일반적으로 대략적인 추론으로 간주되지 않으며 h*의 정확한 값을 계산한다.\n",
    "* 그러나 L(v,h,q)를 최대화 하는 것에 기초한 학습 과정을 개발하고자 한다면, MAP 추론을 q의 값을 제공하는 절차로 생각할 수 있다.\n",
    "* 이러한 의미에서 MAP 추론은 최적의 q를 제공하지 못하기 때문에 근사 추론으로 생각 할 수 있다.\n",
    "<img src=\"./figures/fig2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정확한 추론은 정확한 최적화 알고리즘을 사용하여 제한되지 않은 확률 분포에 대해 q를 최대화 하는 것으로 구성되며, 분포의 집합을 제한함으로써 MAP 추론을 근사 추론의 한 형태로 도출 할 수가 있다.\n",
    "* 특히 q가 Direc 분포를 취할 것을 요구한다.\n",
    "<img src=\"./figures/fig3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이것은 μ를 통해 q를 완전히 제어 할 수 있다는 것을 의미한다.\n",
    "* μ와 함께 변하지 않는 L의 항을 없애고, MAP 추론 문제와 동등한 최적화 문제만을 남긴다.\n",
    "<img src=\"./figures/fig4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MAP 추론은 EM과 유사한 학습 절차를 수행하는데, MAP 추론을 수행해서 h*를 추론하고, log p(h*, v)를 증가시키기 위해 θ를 업데이트 한다.\n",
    "* EM과 마찬가지로 q에 대해 L을 최적화하기 위해 추론하고, θ에 대해 L을 최적화하기 위해 매개 변수 업데이트를 사용한다.\n",
    "* MAP 추론은 일반적으로 deep learning에서 feature 추출과 학습 매카니즘으로 사용되며, 주로 sparse coding model에 사용된다.\n",
    "* sparse coding은 hidden unit에 sparsity를 유발하는 선형 factor model이며, 주로 factorial Laplace prior를 사용한다.\n",
    "<img src=\"./figures/fig5.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그 후에, visible units이 선형 변환을 수행하고 노이즈를 첨가함으로써 생성된다.\n",
    "<img src=\"./figures/fig6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* p(h|v)를 계산하거나 나타내는 것은 어려우며, 모든 hi와 hj의 쌍은 v의 부모이다.\n",
    "* 이 말의 의미는 v가 관찰될 때, 그래픽 모델이 hi와 hj를 연결하는 활성화 path를 포함한다는 것을 의미한다.\n",
    "* 따라서 모든 hidden unit은 p(h | v) 내에 하나의 거대한 clique가 된다.\n",
    "* 만약 모델이 가우시안이라면, 이러한 상호 작용을 공분산 행렬을 통해 효율적으로 모델링할 수 있지만 sparse prior는 이러한 상호작용을 non-gaussian으로 만든다.\n",
    "* p(h|v)는 다루기가 힘들기 때문에 log-likelihood와 gradient 계산도 어렵다.\n",
    "* 따라서 정확한 maximum likelihood learning을 사용할 수 없다.\n",
    "* 대신 우리는 MAP을 추론하고 direc 분포에 의해 정의된 ELBO를 최대화 하기 위한 파라미터를 학습해야 한다.\n",
    "* 학습 셋의 모든 h 벡터를 행렬 H에 연결하고 모든 v 벡터를 행렬 V에 연결하면 sparse coding 학습 과정은 다음과 같이 최소화 된다.\n",
    "<img src=\"./figures/fig7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sparse coding의 대다수 application은 매우 작은 H와 매우 큰 W를 가지는 것을 막기 위해, weight decay를 포함하거나 W에 대한 norm 제약을 포함한다.\n",
    "* H에 대한 최소화와 W에 대한 최소화를 통해 J를 최소화 할 수가 있다.\n",
    "* H에 대한 최소화는 feature-sign search algorithm(Lee et al., 2007)과 같은 특별한 알고리즘을 필요로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
