{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.11 The Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝 세미나 : 이론 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] Determinant (Wikipedia) - https://en.wikipedia.org/wiki/Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of a square matrix, \n",
    "* $det(A)$\n",
    "* is a function mapping matrices to real scalars. \n",
    "* the product of all the eigenvalues of the matrix.\n",
    "* The absolute value of the determinant\n",
    "    - can be thought of as a measure of \n",
    "        - how much \n",
    "            - multiplication \n",
    "                - by the matrix expands \n",
    "            - or contracts space. \n",
    "    - If the determinant is 0, \n",
    "        - then space is contracted completely \n",
    "            - along at least one dimension, \n",
    "            - causing it to lose all of its volume. \n",
    "    - If the determinant is 1, \n",
    "        - then the transformation \n",
    "            - preserves volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Area_parallellogram_as_determinant.svg\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b9/Determinant_parallelepiped.svg\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.12 Example: Principal Components Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] 차원축소 훑어보기 (PCA, SVD, NMF) - http://www.slideshare.net/madvirus/pca-svd\n",
    "* [4] Principal Component Analysis - http://www.slideshare.net/rickwendell/principal-component-analysys\n",
    "* [5] Probabilistic PCA, EM, and more</font> - http://www.slideshare.net/hsharmasshare/probabilistic-pca-em-and-more\n",
    "* [6] 머피's 머신러닝: Latent Linear Model - http://www.slideshare.net/JungkyuLee1/s-latent-linear-model\n",
    "* [7] Lecture 7 (prelude)  Some linear generative models and a coding perspective - https://www.cs.toronto.edu/~hinton/csc2515/notes/lec7pre.ppt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.nlpca.org/fig_pca_principal_component_analysis.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://images.slideplayer.com/15/4559668/slides/slide_8.jpg\" width=400 />\n",
    "             <img src=\"https://i.imgur.com/SOjew3N.png\" width=400 />\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap13.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.cs.ubc.ca/~murphyk/Bayes/Figures/gmka.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap_fa.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap_ppca.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple machine learning algorithm, principal components analysis or PCA can be derived using only knowledge of basic linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a collection of $m$ points $\\{x^{(1)}, . . . , x^{(m)}\\}$ in $R^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we would like to apply <font color=\"red\">lossy compression</font> to these points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can encode these points is to represent a lower-dimensional version of them. \n",
    "* For each point $x^{(i)}∈ R^{n}$ \n",
    "    - we will ﬁnd a corresponding <font color=\"red\">code vector $c^{(i)} ∈ R^l$ </font>.\n",
    "        - If $l$ is smaller than $n$, \n",
    "            - it will take less memory to store the code points than the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding fucntion & decoding fucntion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to ﬁnd \n",
    "* some <font color=\"red\">encoding function</font> that \n",
    "    - produces the code for an input, \n",
    "        - $f(x)=c$, and \n",
    "* a <font color=\"red\">decoding function</font> that \n",
    "    - produces the reconstructed input given its code, \n",
    "        - $x≈g(f (x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is deﬁned by our choice of the decoding function.\n",
    "* Speciﬁcally, to make the <font color=\"red\">decoder very simple</font>, \n",
    "    - we choose to <font color=\"blue\">use matrix multiplication</font> \n",
    "        - to map the code back in to $R^n$. \n",
    "* Let $g(c) = Dc$, \n",
    "    - where $D ∈ R^{n×l}$ \n",
    "        - is the <font color=\"red\">matrix deﬁning the decoding</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the optimal code for this decoder could be a diﬃcult problem. \n",
    "* To keep the encoding problem easy, \n",
    "    - PCA <font color=\"red\">constrains the columns of $D$ to be orthogonal to each other</font>.\n",
    "    - (Note that $D$ is still not technically “an orthogonal matrix” unless $l = n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unit norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the problem as described so far, many solutions are possible, \n",
    "* because we can increase the scale of $D_{:,i}$ \n",
    "    - if we decrease $c_i$ proportionally for all points. \n",
    "\n",
    "To give the problem a unique solution, \n",
    "* we <font color=\"red\">constrain all of the columns of $D$ to have unit norm</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how to generate the optimal code point $c^∗$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to turn this basic idea into an algorithm we can implement, \n",
    "* the ﬁrst thing we need to do is ﬁgure out \n",
    "    - how to generate the optimal code point $c^∗$ foreach input point $x$. \n",
    "* One way to do this is to \n",
    "    - <font color=\"red\">minimize the distance</font> between \n",
    "        - the input point $x$ and \n",
    "        - its reconstruction, $g(c^∗)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  $L^2$ norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/cs445linearalgebraandmatlabtutorial-150831010550-lva1-app6891/95/linear-algebra-and-matlab-tutorial-16-638.jpg?cb=1440983969\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can <font color=\"red\">measure this distance using a norm</font>. \n",
    "* In the principal components algorithm, <font color=\"blue\">we use the $L^2$ norm</font> :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L^2$ norm -> squared $L^2$ norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 \n",
    "* [8] Monotonic Function - http://encyclopedia2.thefreedictionary.com/Monotonically+increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://img.tfd.com/ggse/2c/gsed_0001_0016_0_img4169.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can switch to the squared $L^2$ norm instead of the $L^2$ norm itself, \n",
    "* because both are minimized by the same value of $c$. \n",
    "* Both are minimized by the same value of $c$ \n",
    "    - because the $L^2$ norm is non-negative and \n",
    "        - the squaring operation is monotonically increasing \n",
    "            - for non-negative arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function being minimized simpliﬁes to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by the deﬁnition of the L2norm, equation 2.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by the distributive property)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(because the scalar $g(c)^Tx$ is equal to the transpose of itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the ﬁrst term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now change the function being minimized again, <font color=\"red\">to omit the ﬁrst term</font>, since this term does not depend on $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make further progress, we must substitute in the deﬁnition of g(c):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by the orthogonality and unit norm constraints on D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this optimization problem <font color=\"red\">using vector calculus</font> (see section 4.3 if you do not know how to do this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoder function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we can optimally encode $x$ just using a matrix-vector operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the algorithm eﬃcient: \n",
    "* we can optimally encode $x$ just using a matrix-vector operation. \n",
    "* <font color=\"red\">To encode a vector, we apply the encoder function</font> :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decoder function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a further matrix multiplication, we can also deﬁne the PCA reconstruction operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding matrix $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to choose the encoding matrix D. \n",
    "* To do so, we revisit the idea of minimizing the $L^2$ distance between inputs and reconstructions. \n",
    "* Since we will use the same matrix $D$ to decode all of the points, \n",
    "    - we can <font color=\"red\">no longer consider the points in isolation</font>. \n",
    "* Instead, we must minimize \n",
    "    - the Frobenius norm of the matrix of errors computed over all dimensions and all points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding $D^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive the algorithm for ﬁnding D∗, we will start by considering the case where $l = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the case where $l = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this case, $D$ is just a single vector, $d$. \n",
    "    - Substituting equation 2.67 into equation 2.68 and \n",
    "    - simplifying $D$ into $d$, \n",
    "    \n",
    "the problem reduces to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formulation is the most direct way of performing the substitution, but is not the most <font color=\"red\">stylistically pleasing way</font> to write the equation. \n",
    "* It places the scalar value $d^Tx^{(i)}$ on the right of the vector $d$. \n",
    "* It is more conventional to write scalar coeﬃcients on the left of vector they operate on. \n",
    "* We therefore usually writesuch a formula as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, exploiting the fact that a scalar is its own transpose, as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader should aim to become familiar with such <font color=\"red\">cosmetic rearrangements</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### design matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.stack.imgur.com/VZtEr.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it can be helpful to <font color=\"red\">rewrite the problem in terms of a single design matrix of examples</font>, rather than as a sum over separate example vectors.This will allow us to use more compact notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X ∈ R^{m×n}$ be the matrix deﬁned by \n",
    "* stacking all of the vectors \n",
    "    - describing the points, such that\n",
    "        - $X_{i,:} = x^{(i)^T}$.\n",
    "            \n",
    "We can now rewrite the problem as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disregarding the constraint for the moment, we can <font color=\"red\">simplify the Frobenius norm portion</font> as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(by equation 2.49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(because terms not involving d do not aﬀect the arg min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(because we can cycle the order of the matrices inside a trace, equation 2.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(using the same property again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we re-introduce the constraint:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(due to the constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.22.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap2.12.23.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimization problem may be solved using eigendecomposition. \n",
    "* Speciﬁcally,the optimal $d$ is given by \n",
    "    - the eigenvector of $X^TX$ \n",
    "        - corresponding to the <font color=\"red\">largest eigenvalue</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More generally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This derivation is speciﬁc to the case of $l=1$ and recovers only the ﬁrst principal component. \n",
    "* More generally, when we wish to recover a basis of principal components, \n",
    "    - the matrix $D$ is given by \n",
    "        - the <font color=\"red\"> $l$ eigenvectors</font>\n",
    "            - corresponding to the <font color=\"red\">largest eigenvalues</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear algebra is one of the fundamental mathematical disciplines that isnecessary to understand deep learning. \n",
    "* Another key area of mathematics that is ubiquitous in machine learning is probability theory, presented next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 \n",
    "* [1] 2 Linear Algebra (Deep Learning Book) - http://www.deeplearningbook.org/contents/linear_algebra.html\n",
    "* [2] Determinant (Wikipedia) - https://en.wikipedia.org/wiki/Determinant\n",
    "* [3] 차원축소 훑어보기 (PCA, SVD, NMF) - http://www.slideshare.net/madvirus/pca-svd\n",
    "* [4] Principal Component Analysis - http://www.slideshare.net/rickwendell/principal-component-analysys\n",
    "* [5] Probabilistic PCA, EM, and more</font> - http://www.slideshare.net/hsharmasshare/probabilistic-pca-em-and-more\n",
    "* [6] 머피's 머신러닝: Latent Linear Model - http://www.slideshare.net/JungkyuLee1/s-latent-linear-model\n",
    "* [7] Lecture 7 (prelude)  Some linear generative models and a coding perspective - https://www.cs.toronto.edu/~hinton/csc2515/notes/lec7pre.ppt \n",
    "* [8] Monotonic Function - http://encyclopedia2.thefreedictionary.com/Monotonically+increasing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
