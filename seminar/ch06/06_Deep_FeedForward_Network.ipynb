{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Deep Feedfowrd Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝 세미나 : 이론 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 6.1 Example: Learning XOR\n",
    "* 6.2 Gradient-Based Learning\n",
    "    - 6.2.1 Cost Functions\n",
    "        - 6.2.1.1 Learning Conditional Distributions with Maximum Likelihood\n",
    "        - 6.2.1.2 Learning Conditional Statistics \n",
    "    - 6.2.2 Output Units\n",
    "        - 6.2.2.1 Linear Units for Gaussian Output Distributions\n",
    "        - 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions\n",
    "        - 6.2.2.3 Softmax Units for Multinoulli Output Distributions\n",
    "        - 6.2.2.4 Other Output Types\n",
    "* 6.3 Hidden Units\n",
    "    - 6.3.1 Rectiﬁed Linear Units and Their Generalizations\n",
    "    - 6.3.2 Logistic Sigmoid and Hyperbolic Tangent\n",
    "    - 6.3.3 Other Hidden Units\n",
    "* 6.4 Architecture Design\n",
    "    - 6.4.1 Universal Approximation Properties and Depth\n",
    "    - 6.4.2 Other Architectural Considerations\n",
    "* 6.5 Back-Propagation and Other Diﬀerentiation Algorithms\n",
    "    - 6.5.1 Computational Graphs\n",
    "    - 6.5.2 Chain Rule of Calculus\n",
    "    - 6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\n",
    "    - 6.5.4 Back-Propagation Computation in Fully-Connected MLP\n",
    "    - 6.5.5 Symbol-to-Symbol Derivatives\n",
    "    - 6.5.6 General Back-Propagation\n",
    "    - 6.5.7 Example: Back-Propagation for MLP Training\n",
    "    - 6.5.8 Complications\n",
    "    - 6.5.9 Diﬀerentiation outside the Deep Learning Community\n",
    "    - 6.5.10 Higher-Order Derivatives\n",
    "* 6.6 Historical Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] L1 : From Machine Learning to Deep Learning (Udacity) -  https://www.evernote.com/shard/s142/sh/d8bfa5b8-9bb6-4651-947b-2d5623a75723/a14f4e5fb17aaa63\n",
    "* [3] L1 : Deep Neural Networks (Udacity) https://drive.google.com/file/d/0B3vuuoFuJsKWdFFkMS10N1BpLTg/view\n",
    "* [4] CS231n: Convolutional Neural Networks for Visual Recognition : Image classification and the data-driven approach & k-nearest neighbor & Linear classification I - http://cs231n.stanford.edu/slides/winter1516_lecture2.pdf\n",
    "* [5] CS231n: Convolutional Neural Networks for Visual Recognition : Linear classification II & Higher-level representations, image features & Optimization, stochastic gradient descent - http://cs231n.stanford.edu/slides/winter1516_lecture3.pdf\n",
    "* [6] CS231n: Convolutional Neural Networks for Visual Recognition : Backpropagation & Introduction to neural networks - http://cs231n.stanford.edu/slides/winter1516_lecture4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models are called feedforward because information ﬂows through thefunction being evaluated from x, through the intermediate computations used to deﬁne f, and ﬁnally to the output y. \n",
    "* There are no feedback connections in whichoutputs of the model are feedback into itself. \n",
    "* When feedforward neural networksare extended to include feedback connections, they are called recurrent neuralnetworks, presented in Chapter 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward neural networks are called networks because they are typically represented by composing together many diﬀerent functions.\n",
    "* The model is associatedwith a directed acyclic graph describing how the functions are composed together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, \n",
    "* we might have three functions $f^{(1)}$, $f^{(2)}$, and $f^{(3)}$ connected in a chain, to form f(x) = $f^{(1)}(f^{(2)}(f^{(1)}(x)))$.\n",
    "* $f^{(1)}$ is called the ﬁrst layer of the network, $f^{(2)}$ is called the second layer, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall length of the chain gives the depth of the model. \n",
    "* It is from this terminology that the name “deep learning” arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ﬁnal layer of a feedforward network is called the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During neural network training, we drive f(x) to match $f^∗(x)$. \n",
    "* The training data provides us with noisy, approximate examples of $f^∗(x)$ evaluated at diﬀerent training points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each examplexis accompanied by a label $y ≈ f^∗(x)$.\n",
    "* The training examples specify directly what the output layer must do at each point x; \n",
    "* it must produce a value that is close to y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithm must decidehow to use those layers to produce the desired output, but the training data does not say what each individual layer should do. * Instead, the learning algorithm mustdecide how to use these layers to best implement an approximation of $f^∗$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because the training data does not show the desired output for each of these layers, these layers are called hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, these networks are called neural because they are loosely inspired by neuroscience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal of modern neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, modern neural network research is guided by many mathematical and engineering disciplines, and the goal of neural networks is not to perfectly model the brain. \n",
    "* It is best to think of feedforward networks as function approximation machines that are designed to achieve statistical generalization, occasionally drawing some insights from what we know about the brain, rather than as models of brain function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to understand feedforward networks is to begin with linear models and consider how to overcome their limitations. \n",
    "* Linear models, such as logistic regression and linear regression, are appealing because they may be ﬁt eﬃciently and reliably, either in closed form or with convex optimization. \n",
    "* Linear models also have the obvious defect that the model capacity is limited to linear functions, so the model cannot understand the interaction between any two input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nonlinear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend linear models to represent nonlinear functions of x, we can apply the linear model not to x itself but to a transformed input φ(x), where φis a nonlinear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The question is then how to choose the mapping φ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.One option is to use a very generic φ, such as the inﬁnite-dimensional φ that is implicitly used by kernel machines based on the RBF kernel. \n",
    "* If φ(x) is of high enough dimension, we can always have enough capacity to ﬁt thetraining set, but generalization to the test set often remains poor. \n",
    "* Very generic feature mappings are usually based only on the principle of local smoothness and do not encode enough prior information to solve advanced problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Another option is to manually engineer φ.\n",
    "* Until the advent of deep learning, this was the dominant approach. \n",
    "* This approach requires decades of human eﬀort for each separate task, with practitioners specializing in diﬀerent domains such as speech recognition or computer vision, and with little transfer between domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.The strategy of deep learning is to learn φ. \n",
    "* In this approach, we have a model y = f(x;θ,w) = $φ(x;θ)^{\\top}w$. \n",
    "* We now have parameters θ that we use to learn φ from a broad class of functions, and parameters w that map from φ(x) to the desired output. \n",
    "* This is an example of a deep feedforward network, with φ deﬁning a hidden layer. \n",
    "* This approach is the only one of the three that gives up on the convexity of the training problem, but the beneﬁts outweigh the harms. \n",
    "* In this approach, we parametrize the representation as φ(x;θ) and use the optimization algorithm to ﬁnd the θ that corresponds to a good representation. \n",
    "* If we wish, this approach can capture the beneﬁt of the ﬁrst approach by being highly generic—we do so by using a very broad family φ(x;θ). \n",
    "* This approach can also capture the beneﬁt of the second approach.\n",
    "* Human practitioners can encode their knowledge to help generalization by designing families φ(x;θ) that they expect will perform well. \n",
    "* The advantage is that the human designer only needs to ﬁnd the right general function family rather than ﬁnding precisely the right function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Example: Learning XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR function (“exclusive or”) is an operation on two binary values,x1 and x2. When exactly one of these binary values is equal to 1, the XOR function returns 1. Otherwise, it returns 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat this problem as a regression problem and use a mean squared error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/500px-Linear_least_squares_example2.svg.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example, we will not be concerned with statistical generalization.We want our network to perform correctly on the four points X={ $[0,0]^{\\top}$,  $[0,1]^{\\top}$, $[1,0]^{\\top}$, and $[1,1]^{\\top}$}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After solving the normal equations, we obtain w=0 and b=1/2. The linear model simply outputs 0.5 everywhere. Why does this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speciﬁcally, we will introduce a very simple feedforward network with onehidden layer containing two hidden units. See Fig. 6.2 for an illustration of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f^{(1)}(x;W,c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h=^{f(1)}(x;W,c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=f^{(2)}(h;w,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x;W,c,w,b) = $f^{(2)}(f^{(1)}(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f^{(1)}(x)=W^{\\top}x$ and $f^{(2)}(h)=h^{\\top}w$. Then $f(x)=w^{\\top}W^{\\top}x$. We could represent this function as f(x) = $x^{\\top}w^{\\prime}$ where $w^{\\prime}= Ww$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h=g(W^{\\top}x+c)$,where W provides the weights of a linear transformation and c the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functiong is typically chosen to be a function that is applied element-wise, with $h_{i}=g(x^{\\top}W:,_{i}+c_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rectiﬁed linear unit or ReLU\n",
    "g(z) = max{0, z}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this example, we simply speciﬁed the solution, then showed that it obtained zero error. \n",
    "* In a real situation, there might be billions of model parameters and billions of training examples, so one cannot simply guess the solution as we did here. \n",
    "* <font color=\"red\">Instead, a gradient-based optimization algorithm can ﬁnd parameters thatproduce very little error.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Gradient-Based Learning\n",
    "* 6.2.1 Cost Functions\n",
    "* 6.2.2 Output Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Designing and training a neural network is not much diﬀerent from training any other machine learning model with gradient descent.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.is.quoracdn.net/main-qimg-46a77c77c721ba34283308232a1788c8?convert_to_webp=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">The largest diﬀerence between the linear models we have seen so far and neural networks is that the nonlinearity of a neural network causes most interesting loss functions to become non-convex.  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [7] Convex analysis - https://msampler.wordpress.com/2009/07/08/convex-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://msampler.files.wordpress.com/2009/07/cvx-fun.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stocastic gradient decent VS others(linear equation solver, convex optimization) \n",
    "* This means that neural networks are usually trained by using iterative, gradient-based optimizers that merely <font color=\"red\">drive the cost function to a very low value</font>, rather than the linear equation solvers used to train linear regression models or the convex optimization algorithms with global convergence guarantees used to train logistic regression or SVMs.\n",
    "* Convex optimization converges starting from any initial parameters (in theory—in practice it is very robust but can encounter numerical problems). \n",
    "* Stochastic gradient descent appliedto non-convex loss functions has <font color=\"red\">no such convergence guarantee</font>, and is <font color=\"red\">sensitive to the values of the initial parameters</font>.\n",
    "* <font color=\"blue\">For feedforward neural networks, it is important to initialize all weights to small random values. The biases may be initialized to zero or to small positive values.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/30bf2d42d3a9b0e07dbc03a014f4e36dbc06904f/68747470733a2f2f7261772e6769746875622e636f6d2f7175696e6e6c69752f4d616368696e654c6561726e696e672f6d61737465722f696d61676573466f724578706c616e6174696f6e2f4772616469656e7444657363656e74576974684d75746c69706c654c6f63616c4d696e696d756d2e6a7067\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[16].png\"   />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://43284bdcf41602d9ee3b7f9155623bc4db1a5d55.googledrive.com/host/0B56ak7W-HmqASVBYZTJ1WXVoQUU/img/stochastic.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/nn-150216124050-conversion-gate01/95/neural-networks-overview-11-638.jpg?cb=1424102465\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course, train models such as linear regression and support vector machines with gradient descent too, and in fact this is <font color=\"red\">common when the trainingset is extremely large.</font>\n",
    "* From this point of view, training a neural network is notmuch diﬀerent from training any other model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other machine learning models, <font color=\"red\">to apply gradient-based learning</font> we must choose a <font color=\"blue\">cost function</font>, and we must choose how to <font color=\"blue\">represent the output of the model</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Cost Functions\n",
    "* 6.2.1.1 Learning Conditional Distributions with Maximum Likelihood\n",
    "* 6.2.1.2 Learning Conditional Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aspect of the design of a deep neural network is the <font color=\"red\">choice of the cost function</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### priciple of of maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(y | x;θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In most cases, our <font color=\"red\">parametric model</font> deﬁnes a distributionp(y|x;θ) and we simply use the <font color=\"blue\">principle of maximum likelihood</font>. \n",
    "* This means we use the <font color=\"red\">cross-entropy</font> <font color=\"blue\">between the training data and the model’s predictions</font> as the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://statgen.iop.kcl.ac.uk/media/ml1.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://colah.github.io/posts/2015-09-Visual-Information/img/CrossEntropyDef.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cfile4.uf.tistory.com/image/2645033951991C20193341\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/215implementingdeeplearningusingcudnn-150915052020-lva1-app6892/95/251-implementing-deep-learning-using-cu-dnn-17-638.jpg?cb=1442295596\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/svmvssoftmax.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://images.slideplayer.com/11/3277991/slides/slide_57.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.is.quoracdn.net/main-qimg-9e2d012ef7cb8b29d2bed14d2975c986?convert_to_webp=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qiita-image-store.s3.amazonaws.com/0/100523/195fb6d9-adf6-9166-5c81-08665777032d.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://2.bp.blogspot.com/-EVROdmAZ0No/VoTHrtrREZI/AAAAAAAAAn0/gyKG0BUjst8/s1600/softmax-regression-scalargraph.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1.1 Learning Conditional Distributions with Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1.2 Learning Conditional Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Output Units\n",
    "* 6.2.2.1 Linear Units for Gaussian Output Distributions\n",
    "* 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions\n",
    "* 6.2.2.3 Softmax Units for Multinoulli Output Distributions\n",
    "* 6.2.2.4 Other Output Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.12.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2.1 Linear Units for Gaussian Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2.3 Softmax Units for Multinoulli Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.19.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.22.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.23.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.24.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2.4 Other Output Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.25.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.26.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.27.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.28.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.29.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.30.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.31.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.32.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.33.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.34.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.35.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.36.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.37.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.38.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.39.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.40.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.41.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.42.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.43.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.44.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.45.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.46.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.47.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.48.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.49.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.50.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.51.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.52.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.53.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Hidden Units\n",
    "* 6.3.1 Rectiﬁed Linear Units and Their Generalizations\n",
    "* 6.3.2 Logistic Sigmoid and Hyperbolic Tangent\n",
    "* 6.3.3 Other Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.54.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.55.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 Rectiﬁed Linear Units and Their Generalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.56.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.57.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.58.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Logistic Sigmoid and Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.59.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.60.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.61.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.62.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3 Other Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.63.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.64.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.65.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.66.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.67.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.68.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Architecture Design\n",
    "* 6.4.1 Universal Approximation Properties and Depth\n",
    "* 6.4.2 Other Architectural Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.69.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.70.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.1 Universal Approximation Properties and Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.71.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.72.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.73.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.74.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.75.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.76.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4.2 Other Architectural Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.77.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.78.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Back-Propagation and Other Diﬀerentiation Algorithms\n",
    "* 6.5.1 Computational Graphs\n",
    "* 6.5.2 Chain Rule of Calculus\n",
    "* 6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\n",
    "* 6.5.4 Back-Propagation Computation in Fully-Connected MLP\n",
    "* 6.5.5 Symbol-to-Symbol Derivatives\n",
    "* 6.5.6 General Back-Propagation\n",
    "* 6.5.7 Example: Back-Propagation for MLP Training\n",
    "* 6.5.8 Complications\n",
    "* 6.5.9 Diﬀerentiation outside the Deep Learning Community\n",
    "* 6.5.10 Higher-Order Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.79.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.80.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.81.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.82.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 Chain Rule of Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.83.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.84.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.85.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.3 Recursively Applying the Chain Rule to Obtain Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.86.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.87.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.88.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.89.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.90.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.4 Back-Propagation Computation in Fully-Connected MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.91.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.5 Symbol-to-Symbol Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.92.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.93.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.94.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.95.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.96.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.6 General Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.97.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.98.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.99.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.100.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.101.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.7 Example: Back-Propagation for MLP Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.102.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.103.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.8 Complications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.9 Diﬀerentiation outside the Deep Learning Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.104.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.105.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.10 Higher-Order Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap6.106.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6 Historical Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료\n",
    "* [1] DEEP LEARNING (Yoshua Bengio)- http://www.deeplearningbook.org/\n",
    "* [2] L1 : From Machine Learning to Deep Learning (Udacity) -  https://www.evernote.com/shard/s142/sh/d8bfa5b8-9bb6-4651-947b-2d5623a75723/a14f4e5fb17aaa63\n",
    "* [3] L1 : Deep Neural Networks (Udacity) https://drive.google.com/file/d/0B3vuuoFuJsKWdFFkMS10N1BpLTg/view\n",
    "* [4] CS231n: Convolutional Neural Networks for Visual Recognition : Image classification and the data-driven approach & k-nearest neighbor & Linear classification I - http://cs231n.stanford.edu/slides/winter1516_lecture2.pdf\n",
    "* [5] CS231n: Convolutional Neural Networks for Visual Recognition : Linear classification II & Higher-level representations, image features & Optimization, stochastic gradient descent - http://cs231n.stanford.edu/slides/winter1516_lecture3.pdf\n",
    "* [6] CS231n: Convolutional Neural Networks for Visual Recognition : Backpropagation & Introduction to neural networks - http://cs231n.stanford.edu/slides/winter1516_lecture4.pdf\n",
    "* [7] Convex analysis - https://msampler.wordpress.com/2009/07/08/convex-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
