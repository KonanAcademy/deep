{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Optimization for Training DeepModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝 세미나 : 이론 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "* 8.1 How Learning Diﬀers from Pure Optimization\n",
    "    - 8.1.1 Empirical Risk Minimization\n",
    "    - 8.1.2 Surrogate Loss Functions and Early Stopping\n",
    "    - 8.1.3 Batch and Minibatch Algorithms\n",
    "* 8.2 Challenges in Neural Network Optimization\n",
    "    - 8.2.1 Ill-Conditioning\n",
    "    - 8.2.2 Local Minima\n",
    "    - 8.2.3 Plateaus, Saddle Points and Other Flat Regions\n",
    "    - 8.2.4 Cliﬀs and Exploding Gradients\n",
    "    - 8.2.5 Long-Term Dependencies\n",
    "    - 8.2.6 Inexact Gradients\n",
    "    - 8.2.7 Poor Correspondence between Local and Global Structure\n",
    "    - 8.2.8 Theoretical Limits of Optimization\n",
    "* 8.3 Basic Algorithms\n",
    "    - 8.3.1 Stochastic Gradient Descent\n",
    "    - 8.3.2 Momentum\n",
    "    - 8.3.3 Nesterov Momentum\n",
    "* 8.4 Parameter Initialization Strategies\n",
    "* 8.5 Algorithms with Adaptive Learning Rates\n",
    "    - 8.5.1 AdaGrad\n",
    "    - 8.5.2 RMSProp\n",
    "    - 8.5.3 Adam\n",
    "    - 8.5.4 Choosing the Right Optimization Algorithm\n",
    "* 8.6 Approximate Second-Order Methods\n",
    "    - 8.6.1 Newton’s Method\n",
    "    - 8.6.2 Conjugate Gradients\n",
    "    - 8.6.3 BFGS\n",
    "* 8.7 Optimization Strategies and Meta-Algorithms\n",
    "    - 8.7.1 Batch Normalization\n",
    "    - 8.7.2 Coordinate Descent\n",
    "    - 8.7.3 Polyak Averaging\n",
    "    - 8.7.4 Supervised Pretraining\n",
    "    - 8.7.5 Designing Models to Aid Optimization\n",
    "    - 8.7.6 Continuation Methods and Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] An overview of gradient descent optimization algorithms - http://sebastianruder.com/optimizing-gradient-descent/index.html\n",
    "* [3] Stochastic gradient methods for machine learning - http://research.microsoft.com/en-us/um/cambridge/events/mls2013/downloads/stochastic_gradient.pdf\n",
    "* [4] (OXFORD, Machine Learning: 2014-2015 )Deep Learning Lecture 6: Optimization - https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/lecture5.pdf\n",
    "* [5] 다크프로그래머 : Gradient, Jacobian 행렬, Hessian 행렬, Laplacian- http://darkpgmr.tistory.com/132\n",
    "* [6] 조금은 느리게 살자 : 테일러 급수(級數, Taylor series) - http://ghebook.blogspot.kr/2010/07/taylor-series.html\n",
    "* [7] 다크프로그래머 : 테일러 급수의 이해와 활용 (Taylor series) - http://darkpgmr.tistory.com/59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter presents these optimization techniques for neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.is.quoracdn.net/main-qimg-46a77c77c721ba34283308232a1788c8?convert_to_webp=true\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://msampler.files.wordpress.com/2009/07/cvx-fun.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/30bf2d42d3a9b0e07dbc03a014f4e36dbc06904f/68747470733a2f2f7261772e6769746875622e636f6d2f7175696e6e6c69752f4d616368696e654c6561726e696e672f6d61737465722f696d61676573466f724578706c616e6174696f6e2f4772616469656e7444657363656e74576974684d75746c69706c654c6f63616c4d696e696d756d2e6a7067\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/aihandson20140710bslideshare-140716182318-phpapp02/95/jsais-ai-tool-introduction-deep-learning-pylearn2-and-torch7-52-638.jpg?cb=1435207500\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">This chapter focuses on one particular case of optimization</font>: \n",
    "* ﬁnding the parameters $θ$ of a neural network that signiﬁcantly reduce a cost function $J(θ)$, which typically includes a performance measure evaluated on the entire training set as well as additional regularization terms.\n",
    "    <img src=\"http://cs231n.github.io/assets/dataflow.jpeg\" />\n",
    "* We begin with a description of how optimization used as a training algorithm for a machine learning task diﬀers from pure optimization. \n",
    "* Next, we present several of the concrete challenges that make optimization of neural networks diﬃcult. \n",
    "* We then deﬁne several practical algorithms, including both\n",
    "    - optimization algorithms themselves and \n",
    "    - strategies for initializing the parameters. \n",
    "* More advanced algorithms adapt \n",
    "    - their learning rates during training or \n",
    "    - leverage information contained in the second derivatives of the cost function\n",
    "* Finally, we conclude with a review of \n",
    "    - several optimization strategies that are formed by \n",
    "        - combining simple optimization algorithms \n",
    "            - into higher-level procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 How Learning Diﬀers from Pure Optimization\n",
    "* 8.1.1 Empirical Risk Minimization\n",
    "* 8.1.2 Surrogate Loss Functions and Early Stopping\n",
    "* 8.1.3 Batch and Minibatch Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization algorithms used for training of deep models diﬀer from traditional optimization algorithms in several ways.\n",
    "* Machine learning usually acts indirectly.\n",
    "    - In most machine learning scenarios, \n",
    "        - we care about some performance measure $P$, \n",
    "            - that is deﬁned with respect to the test set and \n",
    "            - may also be intractable. \n",
    "    - We therefore optimize $P$ only indirectly. \n",
    "* We reduce a diﬀerent cost function $J(θ)$ in the hope that \n",
    "    - doing so will improve P. \n",
    "* This is in contrast to pure optimization,where minimizing \n",
    "    - $J$ is a goal in and of itself. \n",
    "* Optimization algorithms for training deep models \n",
    "    - also typically include \n",
    "        - some specialization \n",
    "            - on the speciﬁc structure \n",
    "                - of machine learning objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the cost function can be written as an average over the training set,such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whereLis the per-example loss function, $f(x;θ)$ is the predicted output whenthe input is $x$, $ˆp$ data is the empirical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data generating distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eq. 8.1 deﬁnes an objective function with respect to the training s\n",
    "* We would usually prefer to minimize \n",
    "    - the corresponding objective function \n",
    "        - where the expectation is taken across \n",
    "            - the data generating distribution $p$ data \n",
    "                - rather than just over the ﬁnite training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.1 Empirical Risk Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [8] Empirical Risk Minimization - http://demo.clab.cs.cmu.edu/fa2015-11763/slides/erm.pdf\n",
    "* [9] The Learning Problem and Regularization - http://www.mit.edu/~9.520/spring11/slides/class02.pdf\n",
    "* [10] Risk Minimization - http://hellbell.tistory.com/entry/Risk-Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.svms.org/srm/Rychetsky2001_2-4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to convert a machine learning problem back into an optimization problem is to minimize the expected loss on the training set. \n",
    "* This means replacing the true distribution $p(x, y)$ with the empirical distribution $ˆp(x, y)$ deﬁned by the training set. \n",
    "* We now minimize the empirical risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where m is the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process based on minimizing this average training error is known as <font color=\"red\">empirical risk minimization</font>.\n",
    "* In this setting, machine learning is still very similarto straightforward optimization. \n",
    "* Rather than optimizing the risk directly, \n",
    "    - we optimize the empirical risk, \n",
    "    - and hope that \n",
    "        - the risk decreases signiﬁcantly as well.\n",
    "* A variety of theoretical results establish conditions under which the true risk can be expected to decrease by various amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">However, empirical risk minimization is prone to overﬁtting</font>. \n",
    "* Models with high capacity can simply memorize the training set. In many cases, empirical risk minimization is not really feasible. \n",
    "    - The most eﬀective modern optimizationalgorithms are based on gradient descent, but many useful loss functions, suchas 0-1 loss, have no useful derivatives (the derivative is either zero or undeﬁnedeverywhere). \n",
    "* These two problems mean that, <font color=\"red\">in the context of deep learning, werarely use empirical risk minimization</font>. \n",
    "* Instead, we must use a slightly diﬀerent approach, in which the quantity that we actually optimize is even more diﬀerent from the quantity that we truly want to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.2 Surrogate Loss Functions and Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [11] Surrogate Loss Functions in Machine Learning - http://fa.bianp.net/blog/2014/surrogate-loss-functions-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surrogate Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the loss function we actually care about (say classiﬁcation error) is notone that can be optimized eﬃciently. \n",
    "* For example, exactly minimizing expected 0-1 loss is typically intractable (exponential in the input dimension), even for a linearclassiﬁer (Marcotte and Savard, 1992). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such situations, one typically optimizesa <font color=\"red\">surrogate loss function</font> instead, which acts as a proxy but has advantages.\n",
    "* For example, the negative log-likelihood of the correct class is typically used as asurrogate for the 0-1 loss. \n",
    "* The negative log-likelihood allows the model to estimatethe conditional probability of the classes, given the input, and if the model can do that well, then it can pick the classes that yield the least classiﬁcation error in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://fa.bianp.net/blog/images/2014/loss_functions.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://fa.bianp.net/blog/images/2014/loss_01.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://fa.bianp.net/blog/images/2014/loss_log.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A very important diﬀerence between optimization in general and optimizationas we use it for training algorithms is that training algorithms do not usually haltat a local minimum. \n",
    "* Instead, a machine learning algorithm usually minimizesa surrogate loss function but halts when a convergence criterion based on early stopping (Sec. 7.8) is satisﬁed.  \n",
    "* <font color=\"red\">Typically the early stopping criterion is based on the true underlying loss function, such as 0-1 loss measured on a validation set</font>, and is designed to cause the algorithm to halt whenever overﬁtting begins to occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://documentation.statsoft.com/STATISTICAHelp/SANN/Images/ErrVsTrain.bmp\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://deeplearning4j.org/img/earlystopping.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.3 Batch and Minibatch Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [12] Computer vision: models, learning and inference / Chapter 4 Fitting Probability Models. - http://slideplayer.com/slide/5277039/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization algorithms for machine learning typically compute each update to the parameters based on an <font color=\"red\">expected value of the cost function</font> <font color=\"blue\">estimated using only a subset of</font> the terms of the full cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.onekobo.com/Articles/Statistics/statsImgs/image204c.gif\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, maximum likelihood estimation problems, when viewed in log space, decompose into a sum over each example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://images.slideplayer.com/17/5277039/slides/slide_47.jpg\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing this sum is equivalent to maximizing the expectation over the <font color=\"red\">empirical distribution</font> deﬁned by the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.unc.edu/courses/2007spring/enst/562/001/images/lectures/lecture25/fig2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the properties of the objective function $J$ used by most of our optimization algorithms are also <font color=\"red\">expectations over the training set</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the most commonly used property is the gradient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Computing this expectation exactly is very expensive because it requires evaluating the model on every example in the entire dataset</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more samples ? small number of samples ?\n",
    "* Recall that the standard error of the mean (Eq. 5.46) estimated from n samples is given by σ/√n, where σ is the true standard deviation of the value of the samples.\n",
    "* The denominator of √n shows that there are less than linear returns to using more examples to estimate the gradient. \n",
    "* Compare two hypothetical estimates of the gradient, one based on 100 examples and another based on 10,000 examples.\n",
    "    - <font color=\"red\">The latter requires 100 times more computation than the former, but reduces the standard error of the mean only by a factor of 10</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Most optimization algorithms converge much faster (in terms of total computation, not in terms of number ofupdates) if they are allowed to rapidly compute approximate estimates of the gradient rather than slowly computing the exact gradient.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another consideration motivating statistical estimation of the gradient from a small number of samples is <font color=\"red\">redundancy in the training set</font>.\n",
    "* In the worst case, all $m$ samples in the training set could be identical copies of each other. \n",
    "* A sampling-based estimate of the gradient could compute the correct gradient with a single sample, using $m$ times less computation than the naive approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch (or deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization algorithms that use the entire training set are called batch ordeterministic gradient methods, <font color=\"red\">because they process all of the training examples simultaneously in a large batch</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.slidesharecdn.com/aihandson20140710bslideshare-140716182318-phpapp02/95/jsais-ai-tool-introduction-deep-learning-pylearn2-and-torch7-52-638.jpg?cb=1435207500\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically the term “batch gradient descent”implies the use of the full training set, while the use of the term “batch” to describea group of examples does not. For example, it is very common to use the term“batch size” to describe the size of a minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stochastic (or online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimization algorithms that use only a single example at a time are sometimescalled stochastic or sometimes online methods. \n",
    "* The term online is usually reservedfor the case where the examples are drawn from a stream of continually createdexamples rather than from a ﬁxed-size training set over which several passes aremade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minibatch or minibatch stochastic methods or stochastic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most algorithms used for deep learning fall somewhere in between, using more one but less than all of the training examples. \n",
    "* These were traditionally called minibatch or minibatch stochastic methods and it is now common to simply call them stochastic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch sizes are generally driven by the following factors:\n",
    "* Larger batches provide a more accurate estimate of the gradient, but with less than linear returns.\n",
    "* Multicore architectures are usually underutilized by extremely small batches.\n",
    "    - This motivates using some absolute minimum batch size, below which thereis no reduction in the time to process a minibatch.\n",
    "* If all examples in the batch are to be processed in parallel (as is typically the case), then the amount of memory scales with the batch size. \n",
    "    - For many hardware setups this is the limiting factor in batch size.\n",
    "* Some kinds of hardware achieve better runtime with speciﬁc sizes of arrays.\n",
    "    - Especially when using GPUs, it is common for power of 2 batch sizes to oﬀer better runtime. \n",
    "    - Typical power of 2 batch sizes range from 32 to 256, with 16 sometimes being attempted for large models.\n",
    "* Small batches can oﬀer a regularizing eﬀect (Wilson and Martinez, 2003),perhaps due to the noise they add to the learning process. \n",
    "    - Generalization error is often best for a batch size of 1.\n",
    "    - Training with such a <font color=\"red\">small batch size</font> might require a <font color=\"red\">small learning rate</font> to maintain stability due to the high variance in the estimate of the gradient. \n",
    "    - <font color=\"red\">The total runtime can be very high</font> due to the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diﬀerent kinds of algorithms use <font color=\"blue\">diﬀerent kinds of information from the mini-batch</font> in diﬀerent ways.\n",
    "* Some algorithms are more sensitive to sampling error than others, either \n",
    "    - because they use information that is diﬃcult to estimate accurately with few samples, or \n",
    "    - because they use information in ways that amplify sampling errors more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradient only\n",
    "* Methods that compute updates based only on the gradient $g$ are usually relatively robust and can handle smaller batch sizes like 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second-ordermethods\n",
    "* Second-ordermethods, which use also the Hessian matrixHand compute updates such as $H^{−1}g$, typically require much larger batch sizes like 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minibatches be selected randomly\n",
    "* It is also crucial that the minibatches be selected randomly.\n",
    "    - Computing an unbiased estimate of the expected gradient from a set of samples requires that those samples be independent.\n",
    "    - We also wish for two subsequent gradient estimates to be independent from each other, so two subsequent minibatches of examples should also be independent from each other. \n",
    "    - Many datasets are most naturally arranged in a way where successive examples are highly correlated.\n",
    "* For very large datasets, for example datasets containing billions of examples in a data center, it can be impractical to sample examples truly uniformlyat random every time we want to construct a minibatch.\n",
    "    - <font color=\"red\">Fortunately, in practiceit is usually suﬃcient to shuﬄe the order of the dataset once and then store it in shuﬄed fashion.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Many optimization problems in machine learning decompose over examples well enough that we can compute entire separate updates over diﬀerent examples in parallel. \n",
    "* In other words, we can compute the update that minimizes $J(X)$ for one minibatch of examples $X$ at the same time that we compute the update for several other minibatches. \n",
    "* Such asynchronous parallel distributed approaches are discussed further in Sec. 12.1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://docplayer.net/docs-images/26/7315497/images/71-0.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An interesting motivation for minibatch stochastic gradient descent is that it follows the gradient of the true generalization error(Eq. 8.2) so long as <font color=\"red\">no examples are repeated</font>.\n",
    "    - Most implementations of minibatch stochastic gradient descent shuﬄe the dataset once and then pass through it multiple times. \n",
    "        - On the ﬁrst pass, each minibatch is used to compute an unbiased estimate of the true generalization error. \n",
    "        - On the second pass, the estimate becomes biased because it isformed by re-sampling values that have already been used, rather than obtaining new fair samples from the data generating distribution.\n",
    "* The fact that stochastic gradient descent minimizes generalization error iseasiest to see in the online learning case, where examples or minibatches are drawnfrom a stream of data. \n",
    "    - In other words, instead of receiving a ﬁxed-size training set, the learner is similar to a living being who sees a new example at each instant, with every example (x, y) coming from the data generating distribution $p_{data}(x, y)$.\n",
    "    - In this scenario, examples are never repeated; every experience is a fair sample from $p_{data}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Challenges in Neural Network Optimization\n",
    "* 8.2.1 Ill-Conditioning\n",
    "* 8.2.2 Local Minima\n",
    "* 8.2.3 Plateaus, Saddle Points and Other Flat Regions\n",
    "* 8.2.4 Cliﬀs and Exploding Gradients\n",
    "* 8.2.5 Long-Term Dependencies\n",
    "* 8.2.6 Inexact Gradients\n",
    "* 8.2.7 Poor Correspondence between Local and Global Structure\n",
    "* 8.2.8 Theoretical Limits of Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [13] ICML 2016 tutorials - http://icml.cc/2016/?page_id=97\n",
    "* [14] Recent Advances in Non-Convex Optimization - http://newport.eecs.uci.edu/anandkumar/slides/icml2016-tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training neural networks, we must confront the general non-convex case.\n",
    "\n",
    " In this section,we summarize several of the most prominent challenges involved in optimization for training deep models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.1 Ill-Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [15] 조건수(condition number) -  https://ko.wikipedia.org/wiki/%EC%A1%B0%EA%B1%B4%EC%88%98\n",
    "* [5] 다크프로그래머 : Gradient, Jacobian 행렬, Hessian 행렬, Laplacian- http://darkpgmr.tistory.com/132\n",
    "* [6] 조금은 느리게 살자 : 테일러 급수(級數, Taylor series) - http://ghebook.blogspot.kr/2010/07/taylor-series.html\n",
    "* [7] 다크프로그래머 : 테일러 급수의 이해와 활용 (Taylor series) - http://darkpgmr.tistory.com/59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some challenges arise even when optimizing convex functions. Of these, the most prominent is <font color=\"red\">ill-conditioning of the Hessian matrix $H$ </font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ill-conditioning problem is generally believed to be present in <font color=\"red\">neural network training problems</font>. \n",
    "\n",
    "Ill-conditioning can manifest by <font color=\"red\">causing SGD to get “stuck”</font> in the sense that even very small steps increase the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Though ill-conditioning is present in other settings besides neural network training, some of the techniques used to combat it in other contexts are less applicable to neural networks. \n",
    "* For example, Newton’s method is an excellent tool for minimizing convex functions with poorly conditioned Hessian matrices, \n",
    "    - but in the subsequent sections we will argue that Newton’s method <font color=\"red\">requires signiﬁcant modiﬁcation before it can be applied to neural networks</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.2 Local Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/deep_local_minima.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "[16] The Challenges with Gradient Descent - https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/ch04.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When optimizing a convex function, we know that we have reached a good solution if we ﬁnd a critical point of any kind.\n",
    "* <font color=\"red\">With non-convex functions</font>, such as neural nets, it is possible to have <font color=\"red\">many local minima</font>. Indeed, nearly any deep model is essentially guaranteed to havean extremely large number of local minima. However, as we will see, this is not necessarily a major problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model identiﬁability problem\n",
    "Neural networks and any models with multiple equivalently parametrized latent variables all have multiple local minima because of the model identiﬁability problem.\n",
    "* A model is said to be identiﬁable if a suﬃciently large training set can rule out all but one setting of the model’s parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/rearrangement_invariance.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight space symmetry\n",
    "Models with latent variables are often not identiﬁable because we can obtain equivalent models by exchanging latent variables with each other. \n",
    "* For example, we could take a neural network and modify layer 1 by swapping the incoming weight vector for unit $i$ with the incoming weight vector for unit $j$, then doing the same for the outgoing weight vectors. \n",
    "* If we have $m$ layers with $n$ units each, then there are $n!^{m}$ ways of arranging the hidden units.\n",
    "* This kind of <font color=\"red\">non-identiﬁability is known as weight space symmetry</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-identiﬁability\n",
    "In addition to weight space symmetry, <font color=\"red\">many kinds of neural networks have additional causes of non-identiﬁability</font>. \n",
    "* For example, in any rectiﬁed linear or maxout network, we can scale all of the incoming weights and biases of a unit by $α$ if we also scale all of its outgoing weights by $1/α$. \n",
    "* This means that — <font color=\"red\">if the cost function does not include terms such as weight decay that depend directly on the weights rather than the models’ outputs</font>\n",
    "    - every local minimum of a rectiﬁed linear or maxout network lies on an (m × n)-dimensional hyperbola of equivalent local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### local minima are not a problematic form of non-convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=\"blue\">These model identiﬁability issues mean that there can be an extremely large or even uncountably inﬁnite amount of local minima in a neural network cost function.</font>\n",
    "* However, all of these local minima arising from non-identiﬁability are equivalent to each other in cost function value. As a result, <font color=\"red\">these local minima are not a problematic form of non-convexity</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local minima can be problematic if \n",
    "*  Local minima can be problematic if \n",
    "    - they have high cost in comparison to the global minimum. \n",
    "* One can construct small neural networks, \n",
    "    - even without hidden units, \n",
    "    - that have local minima with higher cost \n",
    "        - than the global minimum\n",
    "* <font color=\"red\">If local minima with high cost are common, this could pose a serious problem for gradient-based optimization algorithms</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### open question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It remains an open question \n",
    "    - whether there are many local minima of high cost for networks of practical interest and \n",
    "    - whether optimization algorithms encounter them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### local minima problem\n",
    "* For many years, most practitioners believed that local minima were a common problem plaguing neural network optimization. \n",
    "* <font color=\"red\">Today, that does not appear to be the case</font>. \n",
    "* The problem remains an active area of research, but experts now suspect that, \n",
    "    - for suﬃciently large neural networks, \n",
    "    - most local minima have a low cost function value, \n",
    "    - and that it is not important to ﬁnd a true global minimum\n",
    "    - <font color=\"red\">rather than to ﬁnd a point in parameter space that has low but not minimal cost</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://nbviewer.jupyter.org/github/KonanAcademy/deep/blob/master/seminar/ch06/figures/fig4.3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.3 Plateaus, Saddle Points and Other Flat Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/1d_critical_points.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saddle point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many high-dimensional non-convex functions, local minima (and maxima)are in fact rare compared to another kind of point with zero gradient: a saddle point. \n",
    "* Some points around a saddle point have greater cost than the saddle point, while others have a lower cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian matrix\n",
    "At a saddle point, \n",
    "* the Hessian matrix has \n",
    "    - both positive and negative eigenvalues. \n",
    "* Points lying along eigenvectors associated with positive eigenvalues have greater cost than the saddle point, \n",
    "    - while points lying along negative eigenvalues have lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/saddle_point.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.4 Cliﬀs and Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.5 Long-Term Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.6 Inexact Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.7 Poor Correspondence between Local and Global Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.15.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2.8 Theoretical Limits of Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 Basic Algorithms\n",
    "* 8.3.1 Stochastic Gradient Descent\n",
    "* 8.3.2 Momentum\n",
    "* 8.3.3 Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.1 Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.17.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.18.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.21.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.22.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.23.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.24.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.25.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.26.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.3 Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.27.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.28.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 Parameter Initialization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.29.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.30.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.5 Algorithms with Adaptive Learning Rates\n",
    "* 8.5.1 AdaGrad\n",
    "* 8.5.2 RMSProp\n",
    "* 8.5.3 Adam\n",
    "* 8.5.4 Choosing the Right Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.1 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.31.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.2 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.32.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.33.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.3 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.35.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.4 Choosing the Right Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.34.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.6 Approximate Second-Order Methods\n",
    "* 8.6.1 Newton’s Method\n",
    "* 8.6.2 Conjugate Gradients\n",
    "* 8.6.3 BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6.1 Newton’s Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.36.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.37.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.38.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6.2 Conjugate Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.40.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.39.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.41.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.42.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.43.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6.3 BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.44.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.45.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.46.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited Memory BFGS (or L-BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.7 Optimization Strategies and Meta-Algorithms\n",
    "* 8.7.1 Batch Normalization\n",
    "* 8.7.2 Coordinate Descent\n",
    "* 8.7.3 Polyak Averaging\n",
    "* 8.7.4 Supervised Pretraining\n",
    "* 8.7.5 Designing Models to Aid Optimization\n",
    "* 8.7.6 Continuation Methods and Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.1 Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.47.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.48.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.49.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.50.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.51.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.52.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.2 Coordinate Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.53.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.3 Polyak Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.54.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.4 Supervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.55.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap8.56.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.5 Designing Models to Aid Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7.6 Continuation Methods and Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"figures/cap8.57.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] DEEP LEARNING (Yoshua Bengio) : 8. Optimization for Training Deep Models - http://www.deeplearningbook.org/contents/optimization.html\n",
    "* [2] An overview of gradient descent optimization algorithms - http://sebastianruder.com/optimizing-gradient-descent/index.html\n",
    "* [3] Stochastic gradient methods\n",
    "for machine learning -  http://research.microsoft.com/en-us/um/cambridge/events/mls2013/downloads/stochastic_gradient.pdf\n",
    "* [4] (OXFORD, Machine Learning: 2014-2015\n",
    ")Deep Learning Lecture 6: Optimization - https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/lecture5.pdf\n",
    "* [5] 다크프로그래머 : Gradient, Jacobian 행렬, Hessian 행렬, Laplacian- http://darkpgmr.tistory.com/132\n",
    "* [6] 조금은 느리게 살자 : 테일러 급수(級數, Taylor series) - http://ghebook.blogspot.kr/2010/07/taylor-series.html\n",
    "* [7] 다크프로그래머 : 테일러 급수의 이해와 활용 (Taylor series) - http://darkpgmr.tistory.com/59\n",
    "* [8] Empirical Risk Minimization - http://demo.clab.cs.cmu.edu/fa2015-11763/slides/erm.pdf\n",
    "* [9] The Learning Problem and Regularization - http://www.mit.edu/~9.520/spring11/slides/class02.pdf\n",
    "* [10] Risk Minimization - http://hellbell.tistory.com/entry/Risk-Minimization\n",
    "* [11] Surrogate Loss Functions in Machine Learning - http://fa.bianp.net/blog/2014/surrogate-loss-functions-in-machine-learning/\n",
    "* [12] Computer vision: models, learning and inference / Chapter 4 Fitting Probability Models. - http://slideplayer.com/slide/5277039/\n",
    "* [13] ICML 2016 tutorials - http://icml.cc/2016/?page_id=97\n",
    "* [14] Recent Advances in Non-Convex Optimization - http://newport.eecs.uci.edu/anandkumar/slides/icml2016-tutorial.pdf\n",
    "* [15] 조건수(condition number) -  https://ko.wikipedia.org/wiki/%EC%A1%B0%EA%B1%B4%EC%88%98\n",
    "* [16] The Challenges with Gradient Descent - https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/ch04.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
