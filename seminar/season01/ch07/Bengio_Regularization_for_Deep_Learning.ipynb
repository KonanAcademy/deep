{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "- 어떻게 하면 training data만이 아니라 new input에 대해서도 잘 동작하게 할 것인가? \n",
    "- \"training err\"가 커지더라도 \"test err\"(또는 \"generalization err\")를 줄일 수 있는 방법들을 통틀어서 **regularization** 이라고 부름.\n",
    "    - 파라미터에 (직접적으로?) 제약을 가하기도 하고,\n",
    "    - (비용 함수에) 추가적인 term을 더해서 (간접적으로) 파라미터 제약을 하기도 하고,\n",
    "    - 여러 가지 모델들의 평균 값을 취하기 한다.(\"ensemble\" method)\n",
    "- \"the true data generating process\"를 찾는다는 것은 전 우주의 도움을 받지 않고는 불가능하다.\n",
    "    - 결국, **적절한 regularization이 가해진 크고 아름다운 모델**이 궁극적으로 우리가 (ML로) 찾으려는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Parameter Norm Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=imgs/eq7-1.png />\n",
    "- 오직 \"weights\"(또는 파라미터)들에만 영향을 주고, \"bias term\"에는 영향을 주지 말아야 함.(**weight decay**)\n",
    "- bias term이 variance에 영향을 주는 것도 적거니와,\n",
    "- bias term까지 제약을 가하면 심각한 underfitting을 유발할 수 있음.\n",
    "- 각 NN 레이어마다 다른 제약을 생각할 수 있으나, 탐색공간 크기를 줄이기 위해 통상 같은 weight decay를 적용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 L2 Parameter Regularization\n",
    "- **ridge regression** or **Tikhonov regression**\n",
    "<img src=imgs/L2-term.png />\n",
    "<img src=imgs/L2-effect.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear regression의 경우... (**inversability**)\n",
    "<img src=imgs/L2-effect-on-linear-regression.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 L1 Regularization\n",
    "<img src=imgs/L1-term.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=imgs/L1-effect.png />\n",
    "<img src=imgs/default.jpg />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Norm Penalties as Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- regularization 파라미터를 최적화 문제로 접근할 수도 있는데...???\n",
    "<img src=imgs/const-opt.png />\n",
    "<img src=imgs/def2.jpg />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 때로는 이런 간접적인 페널티보다 직접적인 제약을 가하는 걸 더 선호하기도 하는데, 이유는...\n",
    "    - 페널티가 최적화 과정에서 local minima에 빠지게 할 수 있기 때문(왜냐하면 θ가 너무 작아져서..)\n",
    "- **Explicit constraints implemented by re-projection**\n",
    "- Hinton et al. (2012c) recommend using constraints combined with\n",
    "**a high learning rate** to allow rapid exploration of parameter space while maintaining\n",
    "some stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Regularization and Under-Constrained Problems\n",
    "- 많은 ML 모델들이 <img src=imgs/xtx.png />가 역행렬을 갖는 것에 의존하는데, <img src=imgs/xtxa.png />는 항상 역행렬을 갖게 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Dataset Augmentation\n",
    "- 가상의 데이터(fake data)를 training set에 집어넣는 것.\n",
    "- object recognition 등의 태스크에 특히 효과적.\n",
    "    - 이미지 픽셀을 이동시키거나, 회전시키거나, 늘리거나 줄이거나...\n",
    "    - 단, '6'을 '9'로 회전시키는 등의 짓은 하지 말자..\n",
    "- input data에 노이즈를 집어넣는 것도 data augmentation의 일종으로 볼 수도 있음.\n",
    "    - Neural networks prove not to be very robust to noise, however (Tang and Eliasmith, 2010).\n",
    "    - One way to improve the robustness of neural networks is simply to train them with random noise applied to their inputs.\n",
    "    - **Dropout**, a powerful regularization strategy that will be described in Sec. 7.12, can be seen as a process of **constructing new inputs by multiplying by noise**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Noise Robustness\n",
    "- noise injection can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units. \n",
    "    - => **dropout**\n",
    "- RNN\n",
    "    - can be interpreted as **a stochastic implementation of a Bayesian inference over the weights.**\n",
    "    - The Bayesian treatment of learning would consider the model weights to be uncertain and \n",
    "    - representable via a probability distribution that reflects this uncertainty. \n",
    "        - Adding noise to the weights is a practical, stochastic way to reflect this uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 Injecting Noise at the Output Targets\n",
    "- 데이터셋의 레이블 자체가 틀려먹을 수 있다. 이런 상태에서 학습시키면 완전 망...\n",
    "    - => 레이블에 대한 노이즈까지 모델링해버리자!\n",
    "        - 적당히 작은 e값에 대해서 y라는 레이블이 정확할 확률을 (1-e)로 할당.\n",
    "        - **label smoothing** regularizes a model based on a softmax with k output values \n",
    "        - by replacing the hard 0 and 1 classification targets with targets of \n",
    "        <img src=imgs/label-smoothing1.png />\n",
    "        <img src=imgs/label-smoothing2.png />\n",
    "        respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Semi-Supervised Learning\n",
    "- Unlabeld P(x) and labeled examples from P(x,y) are used to estimate P(y|x) or predict y from x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Multi-Task Learning\n",
    "- when part of a model is shared across tasks, \n",
    "- that part of the model is more constrained towards good values (assuming the sharing is justified), \n",
    "- often yielding better generalization.\n",
    "<img src=imgs/multi-task-learning.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Early Stopping\n",
    "<img src=imgs/learning-curve.png />\n",
    "<img src=imgs/ea-algo.png />\n",
    "\n",
    "- 파라미터를 많이 저장하고 있을 필요도 없고..\n",
    "- unobtrusive하고..\n",
    "- 다른 방법들과 섞어쓸 수도 있고..\n",
    "- computing cost도 줄여주고..\n",
    "- (단순 linear model에 한정하면) 수학적으로 L2 weight decay와 비슷한 효과를 낸다는 것도 증명할 수 있어.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7.9 Parameter Tying and Parameter Sharing\n",
    "- 기존 도메인 지식이 있으면 그걸 활용하면 좋지 않나?\n",
    "    - 어떤 파라미터들은 서로 연관관계가 강해.. 같은..\n",
    "<img src=imgs/param-share1.png />\n",
    "<img src=imgs/param-share2.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 Sparse Representations\n",
    "- 모델 파라미터가 아니라, **unit activation** 자체에 페널티를 가해보자..\n",
    "    - 이렇게 하면 모델 파라미터에도 간접적으로 영향을 주기는 함.\n",
    "- sparsely **parameterized**\n",
    "<img src=imgs/sp-param.png />\n",
    "- sparse **representation**\n",
    "<img src=imgs/sp-repr.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Bagging and Other Ensemble Methods\n",
    "- **Bagging** (*Bootstrap Aggregating*)\n",
    "    - 여러 모델을 결합해서 generalization err를 줄이는 방법\n",
    "- 각자 독립적인 학습 모델들을 만들고, 결과 투표를 하게 함 => \"**model averaging**\"\n",
    "    - model averaging 전략을 사용하는 기법들을 \"**ensemble methods**\"라고 함.\n",
    "    <img src=imgs/ensemble1.png />\n",
    "    <img src=imgs/ensemble2.png />\n",
    "    <img src=imgs/ensemble3.png />\n",
    "- 강력한 기법이나, benching marking에는 쓰지 마라..\n",
    "    - Netflix Grand Prize 우승 알고리즘도 ensemble methods였음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12 Dropout\n",
    "- 큰 NN에 대해서 ensemble을 적용하려면 비용이 너무 큼\n",
    "- dropout은 현실적으로 bagging을 할 수 있는 방법임.\n",
    "<img src=imgs/dropout1.png />\n",
    "<img src=imgs/dropout2.png />\n",
    "- Typically, an input unit is included with probability 0.8 and a hidden unit is included with probability 0.5\n",
    "- parameter sharing을 통해서 필요 공간을 줄이는 것이 중요한 팩터..\n",
    "- Srivastava et al. (2014) showed that **dropout is more effective than other standard computationally inexpensive regularizers,** such as \n",
    "    - weight decay, filter norm constraints and sparse activity regularization. \n",
    "    - Dropout may also be combined with other forms of regularization to yield a further improvement.\n",
    "- Because dropout is a regularization technique, it reduces the effective capacity of a model. \n",
    "    - To offset this effect, **we must increase the size of the model.**\n",
    "- Wager et al. (2013) showed that, when applied to linear regression, dropout is equivalent to L2 weight decay,\n",
    "    - **For deep models, dropout is not equivalent to weight decay.**\n",
    "- **\"ensemble인듯 ensemble아닌 ensemble같은 bagging...\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.13 Adversarial Training\n",
    "- 인간 수준의 성능을 보이는 NN이라도 정교하게 구성된 테스트케이스(adversarial cases)에 대해서 100% 에러율을 보일 수 있다.\n",
    "- 과도하게 linear한 컴포넌트로 구성된 것도 하나의 원인임(Goodfellow et al. (2014b)\n",
    "<img src=imgs/hyori.jpg />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier\n",
    "<img src=imgs/tangent.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=imgs/adv.png />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
